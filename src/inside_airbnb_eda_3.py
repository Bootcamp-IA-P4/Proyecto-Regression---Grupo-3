# -*- coding: utf-8 -*-
"""inside_airbnb_eda_3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KRhRe3dzSHb5lh_C_reVKLgjMI0jvLum

### **Paso 1: Unificación y Detección de Duplicados**
**Objetivo**: Combinar los 5 archivos CSV en un único DataFrame y verificar si hay duplicados entre los listings.

#### **Acciones a realizar**:
1. **Cargar los archivos CSV**:
"""

# -*- coding: utf-8 -*-
import pandas as pd
from pathlib import Path

# Configurar pandas para mostrar todas las salidas
pd.set_option('display.max_columns', None)  # Mostrar todas las columnas
pd.set_option('display.max_rows', None)     # Mostrar todas las filas
pd.set_option('display.width', 1000)        # Ancho máximo del display
pd.set_option('display.max_colwidth', None) # Mostrar contenido completo de columnas

raw_data_dir = Path("../data/raw/inside/")
processed_data_dir = Path("../data/processed/")
files = [
    raw_data_dir / "listings-03-2024.csv",
    raw_data_dir / "listings-03-2025.csv",
    raw_data_dir / "listings-06-2024.csv",
    raw_data_dir / "listings-12-2024.csv",
]

# Leer y concatenar archivos
print("="*80)
print("1. READING AND CONCATENATING ALL FILES...")
dfs = [pd.read_csv(file) for file in files]
df = pd.concat(dfs, ignore_index=True)
print("✓ Files combined successfully")
print("="*80)


# Función para mostrar secciones claramente
def show_section(title, content, max_lines=20):
    print("\n" + "="*80)
    print(f"{title.upper()}")
    print("="*80)
    if isinstance(content, (pd.DataFrame, pd.Series)):
        with pd.option_context('display.max_rows', max_lines):
            print(content)
    else:
        print(content)

# Mostrar información básica
show_section("3. BASIC DATAFRAME INFORMATION", df.info())

# Mostrar filas
show_section("4. FIRST 5 ROWS", df.head())
show_section("5. LAST 5 ROWS", df.tail())

# Estadísticas
show_section("6. DESCRIPTIVE STATISTICS", df.describe(include='all'))

# Uso de memoria
show_section("7. MEMORY USAGE", df.memory_usage(deep=True))

# Tipos de datos
show_section("8. DATA TYPES", df.dtypes)

# Columnas y forma
show_section("9. COLUMN NAMES", df.columns.tolist())
show_section("10. DATAFRAME SHAPE", f"Rows: {df.shape[0]}, Columns: {df.shape[1]}")

# Valores únicos
show_section("11. UNIQUE VALUES COUNT PER COLUMN", df.nunique())

# Valores nulos
show_section("12. NULL VALUES COUNT PER COLUMN", df.isnull().sum())

# Duplicados
show_section("13. DUPLICATED ROWS COUNT", df.duplicated().sum())

# Valores únicos por columna (detallado)
show_section("14. DETAILED UNIQUE VALUES PER COLUMN", "\n".join([f"{col}: {df[col].nunique()}" for col in df.columns]))

# Valores únicos por fila (primeras 5)
show_section("15. UNIQUE VALUES PER ROW (FIRST 5 ROWS)", "\n".join([f"Row {i}: {df.iloc[i].nunique()}" for i in range(min(5, df.shape[0]))]))

print("\n" + "="*80)
print("✓ ALL DATA HAS BEEN DISPLAYED")
print("="*80)

"""2. **Verificar duplicados**:
   - Usaremos `id` (identificador único de Airbnb) y `scrape_id` (fecha de scraping) para detectar si un mismo listing aparece en múltiples archivos.
"""

# Verificar duplicados basados en 'id' y 'scrape_id'
duplicates = df.duplicated(subset=['id', 'scrape_id'], keep=False)
print(f"Número de filas duplicadas: {duplicates.sum()}")

"""3. **Eliminar duplicados** (si los hay):"""

df = df.drop_duplicates(subset=['id', 'scrape_id'], keep='last')

"""4. **Guardar el DataFrame unificado** (opcional, para no repetir el proceso):"""

# Volcar el dataframe combinado a un archivo CSV
print("="*80)
print("1. SAVING COMBINED DATA TO CSV...")
raw_data_dir.mkdir(parents=True, exist_ok=True)
print(f"✓ Created directory: {raw_data_dir}")
df.to_csv(raw_data_dir / "raw_combined_listings.csv", index=False)
print(f"✓ Saved to {raw_data_dir / 'raw_combined_listings.csv'}")

"""#### **Qué esperamos analizar**:
- ¿Hay listings que aparecen en múltiples archivos? (ej. si un mismo `id` tiene diferentes `scrape_id`).
- ¿Los duplicados son exactos o hay diferencias en columnas como `price`, `number_of_reviews`, etc.?

---

### **Siguiente paso (una vez me compartas los resultados)**:
- Análisis de valores nulos y columnas irrelevantes (eliminaremos las que no aporten al modelo de predicción de precios).
- Estadísticas descriptivas básicas de las columnas numéricas (como `price`, `bedrooms`, etc.).

---

**Por favor, comparte**:
1. El número total de filas después de unificar los archivos.
2. El número de duplicados encontrados (si hubo).
3. ¿Alguna observación interesante? (ej. ¿los duplicados tienen precios diferentes?).

Así ajustamos el siguiente paso basado en estos resultados. ¡Vamos poco a poco!

### **Análisis de los Resultados y Siguiente Paso**

#### **Observaciones Clave:**
1. **Datos Unificados Correctamente**:  
   - Se han combinado **104,996 registros** de los 5 archivos CSV sin duplicados exactos (`duplicates.sum() = 0`).  
   - No hay filas duplicadas (mismo `id` y `scrape_id`), pero hay listings con el mismo `id` en diferentes archivos (ej: actualizaciones temporales).  
---

2. **Problemas Detectados**:  
   - **Columnas con Alta Cardinalidad**:  
     - `amenities` (90,997 valores únicos), `description` (38,189 únicos), `name` (38,541 únicos). Difícil de usar directamente como predictor.  
     - `price` está como texto (ej: `"$31.00"`) y tiene valores nulos (8,2397 no nulos).  
   - **Valores Nulos Relevantes**:  
     - `price` (21.5% nulos), `bathrooms` (21.5% nulos), `bedrooms` (9.3% nulos), `beds` (21.6% nulos).  
     - Variables clave como `review_scores_rating` (20.8% nulos) y `host_response_rate` (19.7% nulos).  

     3. **Columnas con Baja Utilidad**:  
   - URLs (`listing_url`, `picture_url`), IDs (`id`, `scrape_id`), columnas temporales (`last_scraped`, `host_since`).  
   - Columnas redundantes: `host_listings_count` vs `calculated_host_listings_count`.

### **Siguiente Paso: Limpieza Inicial y Preparación de Variables**  
**Objetivo**: Preparar el dataset para el EDA enfocándonos en predictores relevantes para el precio.

#### **Acciones Propuestas**:  
1. **Eliminar Columnas No Relevantess**:
"""

columns_to_drop = [
    'id', 'listing_url', 'scrape_id', 'last_scraped', 'source', 'host_url',
    'host_thumbnail_url', 'host_picture_url', 'calendar_updated',
    'license', 'calendar_last_scraped', 'neighbourhood'  # Redundante con neighbourhood_cleansed
]
df_clean = df.drop(columns=columns_to_drop)

"""2. **Convertir `price` a Numérico**:"""

df_clean['price'] = df_clean['price'].str.replace('$', '').str.replace(',', '').astype(float)
print(f"Price column converted to float. Sample values:\n{df_clean['price'].head()}")

"""3. **Manejar Valores Nulos en Columnas Clave**:  
   - Eliminar filas donde `price` es nulo (es nuestra variable objetivo).  
   - Imputar nulos en `bedrooms`, `bathrooms`, y `beds` con la mediana por `room_type`:
"""

for col in ['bedrooms', 'bathrooms', 'beds']:
    df_clean[col] = df_clean.groupby('room_type')[col].transform(lambda x: x.fillna(x.median()))

"""
4. **Crear Variables Derivadas**:  
   - Extraer el año del host (`host_since`) para usarlo como antigüedad:  """

df_clean['host_since_year'] = pd.to_datetime(df_clean['host_since']).dt.year

"""5. **Filtrar Columnas para Análisis Inicial**:  
   - Seleccionar predictores potenciales basados en relevancia para el precio:
"""

predictors = [
    'neighbourhood_cleansed', 'room_type', 'accommodates', 'bathrooms', 'bedrooms', 'beds',
    'minimum_nights', 'number_of_reviews', 'review_scores_rating', 'instant_bookable',
    'host_is_superhost', 'host_since_year', 'property_type'
]
df_analysis = df_clean[['price'] + predictors]
# Mostrar información del dataframe para análisis
print(f"Dataframe for analysis created with {len(df_analysis)} rows and {len(df_analysis.columns)} columns.")
# Mostrar las primeras filas del dataframe para análisis
print(f"Sample data:\n{df_analysis.head()}")
# Volcar a CSV
df_analysis.to_csv(processed_data_dir / "processed_listings.csv", index=False)
print(f"✓ Saved to {processed_data_dir / 'processed_listings.csv'}")
print("="*80)
print("✓ DATA PREPROCESSING COMPLETED")

"""#### **Qué Esperamos**:  
- Un dataset limpio con `price` como variable numérica y predictores listos para análisis.  
- Reducción de ruido al eliminar columnas irrelevantes.  

---

### **Siguiente Paso: Análisis de Correlaciones y Feature Engineering**  
**Objetivo**: Identificar predictores fuertes, transformar variables y manejar outliers.

#### **1. Análisis Inicial de Correlación**
"""

import seaborn as sns
import matplotlib.pyplot as plt

# Matriz de correlación (solo numéricas)
corr_matrix = df_analysis.corr(numeric_only=True)
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix[['price']].sort_values(by='price', ascending=False), annot=True, cmap='coolwarm')
plt.title("Correlación con Precio")
plt.show()

"""**Qué buscamos**:  
- Variables numéricas con correlación alta (`accommodates`, `bedrooms`, `bathrooms`).  
- Variables categóricas prometedoras (`room_type`, `neighbourhood_cleansed`).

#### **2. Transformaciones Clave**  
**a) Codificar Variables Categóricas**:
"""

# One-Hot Encoding para 'room_type' y 'property_type' (ej: Entire home/apt vs Private room)
df_analysis = pd.get_dummies(df_analysis, columns=['room_type', 'property_type'], drop_first=True)

"""**b) Crear Features Derivadas**:  
- **Densidad de Listings por Barrio**:
"""

neighbourhood_density = df_analysis['neighbourhood_cleansed'].value_counts(normalize=True)
df_analysis['neighbourhood_density'] = df_analysis['neighbourhood_cleansed'].map(neighbourhood_density)

"""- **Antigüedad del Host (años)**:"""

df_analysis['host_experience'] = 2025 - df_analysis['host_since_year']  # Asumiendo año actual 2025

"""**c) Tratar Outliers en `price`**:  """

# Eliminar precios extremos (ej: > percentil 99)
price_upper_limit = df_analysis['price'].quantile(0.99)
df_analysis = df_analysis[df_analysis['price'] <= price_upper_limit]

"""#### **3. Análisis de Distribuciones**  """

import numpy as np

# Distribución de 'price' (log-scale para normalizar)
plt.figure(figsize=(10, 6))
sns.histplot(np.log1p(df_analysis['price']), kde=True)
plt.title("Distribución de Precios (log)")
plt.show()

"""**Acción**: Aplicar log-transform a `price` si la distribución es sesgada.

#### **4. Manejar Texto en `amenities` (Opcional pero Potente)**
"""

# Ejemplo: Crear dummy para amenidades clave (WiFi, Aire Acondicionado)
df_analysis['has_wifi'] = df_clean['amenities'].str.contains('Wifi', case=False).astype(int)
df_analysis['has_air_conditioning'] = df_clean['amenities'].str.contains('Air conditioning', case=False).astype(int)

"""### **Resultado Esperado**  
Un dataframe con:  
- Variables numéricas limpias (`price`, `accommodates`, etc.).  
- Categóricas codificadas (`room_type_Entire home/apt`).  
- Features derivadas (`neighbourhood_density`, `host_experience`).  
- Outliers controlados.  

---

### **Próximos Pasos**  
1. **Ejecutar el código anterior**:  
   - Gráficos de correlación/distribución.  
   - % de nulos restantes.  
2. **Decidir juntos**:  
   - ¿Incluir `amenities` como dummies? (Aumentará dimensionalidad).  
   - ¿Transformar más variables (ej: discretizar `minimum_nights`)?

### **Análisis de los Resultados y Siguientes Acciones**

#### **1. Hallazgos Clave**  
- **Correlaciones Bajas**:  
  - Las variables numéricas tienen correlaciones débiles con `price` (la más alta es `accommodates` con **0.18**).  
  - `review_scores_rating` y `host_experience` casi no impactan (correlación < 0.03).  
  - Variables como `number_of_reviews` y `minimum_nights` muestran correlación negativa insignificante.  

- **Distribución de Precios**:  
  - La distribución de `price` (tras log-transform) sigue una curva cercana a la normal, pero con colas largas (outliers residuales).  

- **Amenidades**:  
  - Features como `has_wifi` o `has_air_conditioning` podrían añadir señal, pero requieren análisis adicional.  

---

#### **2. Problemas Identificados**  
- **Predictores Débiles**: Las variables actuales no explican bien el precio (R² bajo esperable).  
- **Falta de Contexto Geográfico**: `neighbourhood_cleansed` es categórica y no se ha explotado.  
- **Amenidades No Cuantificadas**: Solo se probaron 2 dummies, pero hay más información en el texto.  

---

### **Siguientes Pasos para Mejorar el Modelo**

#### **1. Feature Engineering Avanzado**  
**a) Codificar `neighbourhood_cleansed` con Target Encoding**:  
   - Reemplazar el barrio por el **precio promedio** histórico en esa zona (evita alta dimensionalidad vs One-Hot).
"""

neighbourhood_price = df_analysis.groupby('neighbourhood_cleansed')['price'].mean().to_dict()
df_analysis['neighbourhood_price_avg'] = df_analysis['neighbourhood_cleansed'].map(neighbourhood_price)

"""**b) Crear `avg_score_per_amenity`**:  
   - Calificar amenidades por su "premium" asociado (ej: piscina aumenta un X% el precio).  

"""

amenities_list = ['Pool', 'Air conditioning', 'Wifi', 'Kitchen', 'Washer']
for amenity in amenities_list:
    df_analysis[f'has_{amenity.lower().replace(" ", "_")}'] = df_clean['amenities'].str.contains(amenity, case=False).astype(int)

"""**c) Discretizar `minimum_nights`**:  
   - Convertir en categorías: `short_stay` (<7 noches), `medium_stay` (7-30), `long_stay` (>30).
"""

bins = [0, 7, 30, np.inf]
labels = ['short_stay', 'medium_stay', 'long_stay']
df_analysis['stay_type'] = pd.cut(df_analysis['minimum_nights'], bins=bins, labels=labels)
df_analysis = pd.get_dummies(df_analysis, columns=['stay_type'], drop_first=True)

"""---
#### **2. Reducción de Outliers**  
**a) Eliminar el 1% Extremo en `price`**:  
"""

lower = df_analysis['price'].quantile(0.01)
upper = df_analysis['price'].quantile(0.99)
df_analysis = df_analysis[(df_analysis['price'] >= lower) & (df_analysis['price'] <= upper)]

"""**b) Transformación Logarítmica de `price`**:"""

df_analysis['log_price'] = np.log1p(df_analysis['price'])

"""#### **3. Análisis Visual para Validar Relaciones**  
**a) Boxplot de `price` por `room_type`**:

--------------------------------------------------------------------------------
**Problema con el boxplot de room_type_Entire_apt**
"""

df_analysis.head()

# Mostrar las columnas del dataframe de análisis
print(f"Dataframe for analysis created with {len(df_analysis)} rows and {len(df_analysis.columns)} columns.")

# Imprimir nombres de las columnas
print(f"Column names: {df_analysis.columns.tolist()}")

# Comprobamos si df_clean tiene la columna 'room_type'
print(f"Does df_clean have 'room_type' column? {'room_type' in df_clean.columns}")

# Añadimos la columna 'room_type' al dataframe de análisis
df_analysis['room_type'] = df_clean['room_type']  # Asumiendo que df_clean tiene la columna original

# Comprobamos nueva columna añadida 'room_type' a df_analysis
df_analysis.head()

# Lista de columnas dummy de room_type
room_dummies = ['room_type_Hotel room', 'room_type_Private room', 'room_type_Shared room']

# Crear columna para Entire home/apt (1 si todas las demás son 0)
df_analysis['room_type_Entire_home_apt'] = (df_analysis[room_dummies].sum(axis=1) == 0).astype(int)

df_analysis.head()

"""**Solucionado**

---
"""

sns.boxplot(x='room_type_Entire_home_apt', y='price', data=df_analysis)
plt.title("Precio por Tipo de Alojamiento")
plt.show()

"""**b) Scatterplot de `accommodates` vs `price`**:  """

sns.scatterplot(x='accommodates', y='price', hue='neighbourhood_price_avg', data=df_analysis)
plt.title("Capacidad vs Precio (Color por Barrio)")
plt.show()

"""### **Resultado Esperado**  
Un dataframe con:  
- **Variables más informativas**:  
  - `neighbourhood_price_avg` (geografía como señal numérica).  
  - Amenidades clave como dummies (`has_pool`, `has_air_conditioning`).  
  - `stay_type_medium_stay` y `stay_type_long_stay` (impacto de estancia mínima).  
- **Target (`price`) normalizado** y sin outliers extremos.

---

### **Próximos Pasos**  
1. **Ejecuta el código de transformación** y comparte:  
   - Nuevas correlaciones (¿mejoró la señal de `neighbourhood_price_avg`?).  
   - Gráficos de `room_type` y `accommodates`.  
2. **Decidir juntos**:  
   - ¿Incluir interacciones (ej: `accommodates * room_type`)?  
   - ¿Probamos modelos básicos (Linear Regression, Random Forest) para ver importancia de features?

### **Análisis de las Gráficas Generadas**

#### **1. Boxplot: Precio por Tipo de Alojamiento**  
- **Interpretación**:  
  - Los listings de tipo `Entire home/apt` (valor `1`) tienen una mediana de precio claramente más alta que otros tipos (`Private room`, `Shared room`, etc.).  
  - Hay outliers en ambos grupos, especialmente en `Entire home/apt`, lo que sugiere la presencia de propiedades de lujo o ubicaciones premium.  

- **Acción Recomendada**:  
  - **Mantener esta variable** en el modelo: es un predictor fuerte (como ya sospechábamos).  
  - **Tratar outliers**: Aplicar un límite superior (ej: percentil 95) para evitar que distorsionen el modelo.
"""

# Ejemplo: Filtrar outliers de precio
price_upper_limit = df_analysis['price'].quantile(0.95)
df_filtered = df_analysis[df_analysis['price'] <= price_upper_limit]

"""---

#### **2. Scatterplot: Capacidad (`accommodates`) vs Precio (Color por Barrio)**  
- **Interpretación**:  
  - Relación positiva entre `accommodates` y `price`, pero no lineal (ej: propiedades para 6+ personas pueden tener precios similares a las de 4-5).  
  - Los colores (barrios) muestran que la ubicación también impacta: algunos barrios tienen precios consistentemente más altos independientemente de la capacidad.

- **Acción Recomendada**:  
  - **Crear interacciones**: Combinar `accommodates` con `neighbourhood_price_avg` para capturar cómo el precio por persona varía por zona.  
  - **Discretizar capacidad**: Agrupar en categorías como `small` (1-2), `medium` (3-5), `large` (6+).
"""

# Interacción entre accommodates y barrio
df_analysis['price_per_person'] = df_analysis['price'] / df_analysis['accommodates']

# Discretizar accommodates
df_analysis['accommodates_group'] = pd.cut(
    df_analysis['accommodates'],
    bins=[0, 2, 5, np.inf],
    labels=['small', 'medium', 'large']
)

"""---

### **Próximos Pasos para Mejorar el Modelo**

#### **1. Feature Engineering Adicional**  
- **Amenidades Premium**:  
  Usar las columnas `has_pool`, `has_air_conditioning`, etc., para crear un **"score de lujo"** (suma de amenidades premium).
"""

amenities_premium = ['has_pool', 'has_air_conditioning', 'has_washer']
df_analysis['luxury_score'] = df_analysis[amenities_premium].sum(axis=1)

"""- **Antigüedad del Host**:  
  Transformar `host_since_year` en categorías (ej: `new_host` (<2 años), `experienced_host` (2-5), `veteran_host` (>5)).

#### **2. Correlaciones Actualizadas**  
Ejecuta una nueva matriz de correlación incluyendo las nuevas variables (`price_per_person`, `luxury_score`, etc.):
"""

corr_matrix = df_analysis.corr(numeric_only=True)
plt.figure(figsize=(8, 8))
sns.heatmap(corr_matrix[['price']].sort_values(by='price', ascending=False), annot=True, cmap='coolwarm')

"""**Solución al Error**:

Transformación de neighbourhood_cleansed
El error ocurre porque RandomForestRegressor no puede manejar directamente variables categóricas como strings (ej: 'Cármenes'). Necesitamos codificar la columna neighbourhood_cleansed en un formato numérico. Aquí tienes las opciones:

**Opción 1**:

**Target Encoding** (Recomendado para barrios)
Reemplaza cada barrio por el precio promedio histórico en esa zona. Esto captura la relación entre ubicación y precio sin añadir alta dimensionalidad.
"""

# Calcular precio promedio por barrio
neighbourhood_avg_price = df_analysis.groupby('neighbourhood_cleansed')['price'].mean().to_dict()

# Crear nueva columna numérica
df_analysis['neighbourhood_encoded'] = df_analysis['neighbourhood_cleansed'].map(neighbourhood_avg_price)

# Eliminar la columna original de strings
df_analysis.drop(columns=['neighbourhood_cleansed'], inplace=True)

df_analysis.head()

"""### **Solución al Error: Conversión de Variables Categóricas/Strings a Numéricas**

El error ocurre porque `RandomForestRegressor` no puede manejar directamente columnas con strings (`'f'`, `'t'`) o booleanos (`True`, `False`). Necesitamos convertir **todas las columnas no numéricas** a formato numérico. Aquí está cómo hacerlo:

---

### **1. Identificar Columnas Problemáticas**
Primero, verifica qué columnas no son numéricas:
"""

print(df_analysis.dtypes)

"""- Las columnas con `object`, `bool` o strings (ej: `'f'`, `'t'`) deben transformarse.

***

### **Solución al Error: Manejo de Valores `NaN` en Columnas Booleanas**

El error ocurre porque algunas de tus columnas booleanas contienen valores `NaN` (nulos), y al intentar convertirlas directamente a enteros (`int`), Python no sabe cómo manejar estos valores nulos. Aquí te muestro cómo solucionarlo:
"""

# Sacar las columnas boleanas
bool_columns = df_analysis.select_dtypes(include=['bool']).columns.tolist()
print(f"Boolean columns: {bool_columns}")

"""---

### **Paso 1: Identificar Columnas con Valores Nulos**
Primero, verifica qué columnas tienen valores nulos:
"""

print(df_analysis[bool_columns].isnull().sum())

"""---

### **Paso 2: Estrategia para Manejar `NaN`**
Tienes dos opciones para manejar los valores nulos en columnas booleanas:

#### **Opción A: Rellenar `NaN` con un Valor por Defecto (ej: 0)**
"""

for col in bool_columns:
    if col in df_analysis.columns:
        # Rellenar NaN con 0 y luego convertir
        df_analysis[col] = (
            df_analysis[col]
            .fillna(0)  # Rellenar NaN con 0 (o 1 si prefieres)
            .astype(str)
            .replace({'t': 1, 'f': 0, 'True': 1, 'False': 0})
            .astype(int)
        )

df_analysis.head()

"""#### **Opción B: Eliminar Filas con `NaN` en Columnas Booleanas**"""

# df_analysis.dropna(subset=bool_columns, inplace=True)

"""---

### **Paso 3: Convertir Columnas Booleanas (`True`/`False`) a `1`/`0`**
Las columnas como `stay_type_medium_stay` y `stay_type_long_stay` son de tipo `bool` (no `object`), así que puedes convertirlas directamente:
"""

bool_cols_non_object = ['stay_type_medium_stay', 'stay_type_long_stay'] + \
                      [col for col in df_analysis.columns if df_analysis[col].dtype == bool]

for col in bool_cols_non_object:
    if col in df_analysis.columns:
        df_analysis[col] = df_analysis[col].astype(int)

"""---

### **Paso 4: Eliminar Columnas Redundantes**
Elimina la columna `room_type` (ya tienes las dummies):
"""

df_analysis.drop(columns=['room_type'], inplace=True, errors='ignore')

"""### **Paso 5: Verificar Tipos de Datos Finales**"""

print(df_analysis.dtypes)

"""- Asegúrate de que todas las columnas sean `int64`, `float64`, o `category`."""

df_analysis['host_is_superhost'] = df_analysis['host_is_superhost'].fillna('f')

df_analysis['host_is_superhost'] = df_analysis['host_is_superhost'].replace({'t': 1, 'f': 0}).astype(int)

df_analysis['host_is_superhost'] = (
    df_analysis['host_is_superhost']
    .replace({'t': 1, 'f': 0})
    .infer_objects(copy=False)
    .astype(int)
)

# instant_bookable

df_analysis['instant_bookable'] = (
    df_analysis['instant_bookable']
    .replace({'t': 1, 'f': 0})
    .infer_objects(copy=False)
    .astype(int)
)

print(df_analysis['host_is_superhost'].unique())

print(df_analysis['accommodates_group'].unique())

df_analysis = pd.get_dummies(
    df_analysis,
    columns=['accommodates_group'],
    prefix='acc_group',
    dtype=int  # Esto asegura que obtienes 1 y 0
)

# Elimina una columna para evitar multicolinealidad
df_analysis.drop(columns=['acc_group_small'], inplace=True)

# Imprimir las primeras filas del dataframe de análisis
print(f"Dataframe for analysis created with {len(df_analysis)} rows and {len(df_analysis.columns)} columns.")
# Imprimir las primeras filas del dataframe de análisis
print(f"Sample data:\n{df_analysis.head()}")

# Contar las columnas totales
total_columns = len(df_analysis.columns)
print(f"Total columns in df_analysis: {total_columns}")
# recoger todas las columnas
all_columns = df_analysis.columns.tolist()
print(f"All columns in df_analysis: {all_columns}")

# Volcar df_analysis a CSV
# Asegúrate de que el directorio existe
processed_data_dir = Path("../data/processed/")
processed_data_dir.mkdir(parents=True, exist_ok=True)
df_analysis.to_csv(processed_data_dir / "df_analysis.csv", index=False)
print(f"✓ Saved to {processed_data_dir / 'df_analysis.csv'}")
print("="*80)
print("✓ DATA PREPROCESSING COMPLETED")

"""---

### **Comprobación del dataframe con MODELOS**
---

"""

# from sklearn.ensemble import RandomForestRegressor

# X = df_analysis.drop(columns=['price', 'log_price'])  # Features
# y = df_analysis['price']  # Target

# model = RandomForestRegressor(n_estimators=100, random_state=42)
# model.fit(X, y)

"""---

### **Notas Adicionales**
1. **Si persisten errores**:  
   - Verifica que no queden columnas no numéricas con `print(df_analysis.dtypes)`.  
   - Si hay columnas `category` (como `accommodates_group`), conviértelas a dummies:

2. **Estrategia para `NaN`**:  
   - Si decides rellenar con `0`, asegúrate de que no distorsione el análisis (ej: `NaN` en `host_is_superhost` podría interpretarse como "no superhost").  

3. **Columnas con muchos `NaN`**:  
   - Si una columna tiene muchos `NaN`, considera eliminarla o imputar valores basados en otras variables.

---

### **Ejemplo de DataFrame Corregido**
| price | accommodates | ... | instant_bookable (1/0) | host_is_superhost (1/0) | ... | stay_type_medium_stay (1/0) |
|-------|--------------|-----|------------------------|--------------------------|-----|-----------------------------|
| 100   | 2            | ... | 1                      | 0                        | ... | 1                           |

¡Con estos cambios, tu modelo debería ejecutarse correctamente! Si necesitas más ajustes, dime qué columnas específicas aún dan problemas.

### **4. Verificar que Todas las Columnas sean Numéricas**
"""

print(df_analysis.dtypes)

"""- Asegúrate de que **todas** las columnas sean `int64`, `float64`, etc. Si alguna no lo es, repite los pasos anteriores.

---

### **Notas Adicionales**
- **`neighbourhood_encoded`**: Si usaste Target Encoding para los barrios, asegúrate de que sea `float64`.  
- **Columnas con pocos valores únicos**: Si hay columnas como `accommodates_group` (`small`, `medium`, `large`), aplícales `pd.get_dummies()` o `LabelEncoder`.  
- **Valores nulos**: Si el modelo aún falla, verifica nulos con `df_analysis.isnull().sum()` y elimínalos o imputa valores.

---

### **Ejemplo de DataFrame Listo para Modelar**
| price | accommodates | bathrooms | ... | instant_bookable (1/0) | host_is_superhost (1/0) | ... |
|-------|--------------|-----------|-----|------------------------|--------------------------|-----|
| 100   | 2            | 1.0       | ... | 1                      | 0                        | ... |

---

### **¿Por qué Funciona Esto?**
- Los modelos de sklearn requieren que **todas las features sean numéricas**.  
- Las conversiones a `1`/`0` o valores continuos (Target Encoding) mantienen la información sin perder significado.  

Si el error persiste, comparte el resultado de `df_analysis.dtypes` para revisar columnas específicas.
"""

# from sklearn.ensemble import RandomForestRegressor

# # Separar features y target
# X = df_analysis.drop(columns=['price', 'log_price'])  # Excluir target
# y = df_analysis['price']

# # Entrenar modelo
# model = RandomForestRegressor(n_estimators=100, random_state=42)
# model.fit(X, y)

# # Importancia de features
# pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False).head(10)

"""---

### **Resumen de Acciones**  
1. **Filtrar outliers** de precio (percentil 95).  
2. **Crear interacciones**: `price_per_person` y `accommodates_group`.  
3. **Generar nuevas features**: `luxury_score` y categorías de antigüedad del host.  
4. **Actualizar correlaciones** y probar un modelo inicial.  

¿Quieres que profundicemos en alguno de estos pasos? Por ejemplo, ¿prefieres priorizar el análisis de outliers o el feature engineering?
"""

# Correlación de todas las variables con 'price'
correlations = df_analysis.corr()['price'].abs().sort_values(ascending=False)
print(correlations.head(10))

"""### **Comprobamos que todas las columnas del dataframe `df_analysis` sean numéricas**"""

# Comprobamos las columnas del dataframe de análisis
print(f"Dataframe for analysis created with {len(df_analysis)} rows and {len(df_analysis.columns)} columns.")
# Imprimir nombres de las columnas
print(f"Columnas en df_analysis: {df_analysis.columns.tolist()}")
# Imprimir las primeras filas del dataframe para análisis
df_analysis.head()

"""# **ENTRENAMIENTO Y VALIDACIÓN DE MODELOS**"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import numpy as np

# --- 0. Preparación de los datos ---
# Separar características y objetivo
# 1. Crear copia para trabajar (seguridad)
df_leakage = df_analysis.copy()

# 2. Eliminar columnas problemáticas
columns_to_drop = ['luxury_score', 'price_per_person', 'neighbourhood_price_avg', 'log_price']
df_leakage = df_leakage.drop(columns=columns_to_drop)

# 3. Guardar DataFrame limpio (opcional, pero buena práctica)
df_leakage.to_csv('../data/processed/data_leakage.csv', index=False)

# 4. Definir X e y
X = df_leakage.drop(columns='price')
y = df_leakage['price']

# --- 1. Definición de la grilla de parámetros ---
param_grid_optimized = {
    'max_depth': [10, 15, 20],
    'min_samples_leaf': [2, 3, 4],
    'n_estimators': [100, 200],
    'max_features': ['sqrt', 0.5]
}

# --- 2. División de los datos ---
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# --- 3. Grid Search con validación cruzada ---
grid_search_optimized = GridSearchCV(
    RandomForestRegressor(random_state=42),
    param_grid_optimized,
    cv=5,
    scoring='r2',
    n_jobs=-1
)
grid_search_optimized.fit(X_train, y_train)

# --- 4. Mejor modelo encontrado ---
best_model_optimized = grid_search_optimized.best_estimator_

# --- 5. Predicciones ---
y_pred_train = best_model_optimized.predict(X_train)
y_pred_test = best_model_optimized.predict(X_test)

# --- 6. Función para mostrar métricas ---
def print_metrics(y_true, y_pred, dataset_name=""):
    r2 = r2_score(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)

    print(f"\n📊 Métricas para {dataset_name}")
    print(f"R²:    {r2:.4f}")
    print(f"MSE:   {mse:.2f}")
    print(f"RMSE:  {rmse:.2f}")
    print(f"MAE:   {mae:.2f}")

# --- 7. Resultados finales ---
print("✅ Mejores parámetros encontrados:", grid_search_optimized.best_params_)

print_metrics(y_train, y_pred_train, "Entrenamiento")
print_metrics(y_test, y_pred_test, "Prueba")

"""### 📊 **Análisis de tus Resultados Actuales**
- **R² en entrenamiento (0.895) vs prueba (0.763):**  
  - Hay un **sobreajuste moderado** (diferencia del ~13% entre train y test).  
  - El modelo generaliza decentemente, pero puede mejorar.  

- **Error absoluto (MAE):**  
  - En test, el error promedio es de **21.9 unidades monetarias**.  
  - Para un negocio como Airbnb, esto puede ser aceptable o no, dependiendo del rango de precios (ej: si el precio promedio es 100€, un MAE de 21.9 es alto).  

- **Consistencia del modelo:**  
  - Los parámetros óptimos tienen `max_depth=20` y `n_estimators=200`, lo que sugiere un modelo complejo. Podría simplificarse para reducir overfitting.  

---

### 🚀 **Estrategias para Mejorar el Modelo**
#### **1. Reducir el Sobreajuste**  
- **Aumentar `min_samples_leaf`:** Fuerza a que cada hoja del árbol tenga más muestras, reduciendo complejidad.  
- **Limitar `max_depth`:** Prueba valores menores (ej: 10, 15).  
- **Usar `max_samples`:** Entrena cada árbol con un subconjunto aleatorio de datos (menos correlación entre árboles).  

#### **2. Ingeniería de Features**  
- **Agrupar categorías poco frecuentes** en `property_type` o `room_type` (ej: "Other").  
- **Crear interacciones entre features** (ej: `bedrooms` × `bathrooms`).  

#### **3. Transformación de Target**  
- Si `price` tiene cola larga, aplica `np.log1p(y)` para normalizar y mejorar el R².  

#### **4. Otros Modelos**  
- Prueba **Gradient Boosting (XGBoost, LightGBM)**, que suelen generalizar mejor.  

---


### 📌 **Resultados Esperados**  
- **R² test más cercano a train** (ej: 0.80–0.85 vs 0.89 en train).  
- **MAE reducido** (ej: 18–20 en test).  
- **Modelo más robusto** (menos sensible a ruido).  

---

### 📈 **Pasos Adicionales (si aún hay margen de mejora)**  
1. **Prueba XGBoost:**  
   ```python
   from xgboost import XGBRegressor
   xgb_model = XGBRegressor(random_state=42, tree_method='hist')
   xgb_model.fit(X_train, y_train)
   y_pred_test_xgb = xgb_model.predict(X_test)
   print_metrics(y_test, y_pred_test_xgb, "Prueba (XGBoost)")
   ```

2. **Transformación logarítmica de `y`:**  
   ```python
   y_train_log = np.log1p(y_train)
   y_test_log = np.log1p(y_test)
   model_log = RandomForestRegressor(**grid_search_v2.best_params_)
   model_log.fit(X_train, y_train_log)
   y_pred_test_log = np.expm1(model_log.predict(X_test))  # Revertir transformación
   print_metrics(y_test, y_pred_test_log, "Prueba (Log Transform)")
   ```

3. **Ensamblaje de modelos** (RandomForest + XGBoost).  

---

### 🎯 **Conclusión**  
El modelo actual es decente, pero con **parámetros más restrictivos** y **transformaciones de datos**, puedes lograr:  
- **+5–10% en R² test**  
- **Errores absolutos (MAE) más bajos**.

### 🔥 **Código Mejorado (con todas las validaciones)**
"""

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

# --- 0. Copia del DataFrame y limpieza ---
df_optimized = df_analysis.copy()
columns_to_drop = ['luxury_score', 'price_per_person', 'neighbourhood_price_avg', 'log_price']
df_optimized = df_optimized.drop(columns=columns_to_drop)

# --- 1. Definir X e y ---
X = df_optimized.drop(columns='price')
y = df_optimized['price']

# --- 2. División train-test ---
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# --- 3. Nueva grilla de parámetros (enfoque anti-overfitting) ---
param_grid_optimized_v2 = {
    'max_depth': [10, 15],  # Reducir profundidad máxima
    'min_samples_leaf': [3, 5],  # Aumentar muestras por hoja
    'n_estimators': [100, 150],  # Menos árboles
    'max_features': ['sqrt', 0.3],  # Menos features por árbol
    'max_samples': [0.8, None]  # Entrenar con 80% de datos por árbol
}

# --- 4. Búsqueda con validación cruzada ---
grid_search_v2 = GridSearchCV(
    RandomForestRegressor(random_state=42),
    param_grid_optimized_v2,
    cv=5,
    scoring='r2',
    n_jobs=-1
)
grid_search_v2.fit(X_train, y_train)

# --- 5. Mejor modelo y predicciones ---
best_model_v2 = grid_search_v2.best_estimator_
y_pred_train_v2 = best_model_v2.predict(X_train)
y_pred_test_v2 = best_model_v2.predict(X_test)

# --- 6. Métricas ---
def print_metrics(y_true, y_pred, dataset_name=""):
    r2 = r2_score(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)

    print(f"\n📊 Métricas para {dataset_name}")
    print(f"R²:    {r2:.4f}")
    print(f"MSE:   {mse:.2f}")
    print(f"RMSE:  {rmse:.2f}")
    print(f"MAE:   {mae:.2f}")

print("✅ Mejores parámetros (v2):", grid_search_v2.best_params_)
print_metrics(y_train, y_pred_train_v2, "Entrenamiento (v2)")
print_metrics(y_test, y_pred_test_v2, "Prueba (v2)")

# --- 7. Feature Importances ---
importances = best_model_v2.feature_importances_
feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': importances})
print("\n🔝 Top 10 Features más importantes:")
print(feature_importance_df.sort_values(by='Importance', ascending=False).head(10))

"""### 🔍 **Análisis de Resultados Actuales**
Los resultados muestran:
- **R² test: 0.7185** (ligera mejora en generalización vs modelo anterior, pero aún hay margen).  
- **MAE test: 24.63** (error absoluto alto para precios bajos/medios).  
- **Feature Importances**: `bedrooms`, `accommodates`, y `bathrooms` son las más relevantes (coherente).  

---

### 🛠 **Pasos para Mejorar el Modelo**

#### **1. Transformación Logarítmica del Target (`price`)**
**¿Por qué?**  
Si `price` tiene una distribución asimétrica (cola larga), aplicar `log1p` puede normalizarla y mejorar el modelo.  


**Si la distribución original está muy sesgada**, usa `y_log = np.log1p(y)` en el modelo.
"""

import matplotlib.pyplot as plt

# Histograma de 'price'
plt.figure(figsize=(10, 4))
plt.subplot(1, 2, 1)
plt.hist(y, bins=50, color='blue', alpha=0.7)
plt.title("Distribución Original de 'price'")

# Histograma de 'log(price + 1)'
plt.subplot(1, 2, 2)
plt.hist(np.log1p(y), bins=50, color='green', alpha=0.7)
plt.title("Distribución Logarítmica de 'price'")
plt.show()

"""#### **2. Ingeniería de Features**
##### **A. Agrupar categorías poco frecuentes**  
**Ejemplo para `property_type`:**  
```python
# Contar frecuencias de cada categoría
property_counts = df_optimized['property_type'].value_counts()

# Identificar categorías con menos del 1% de los datos
rare_categories = property_counts[property_counts / len(df_optimized) < 0.01].index

# Agruparlas como "Other"
df_optimized['property_type'] = df_optimized['property_type'].replace(rare_categories, 'Other')
```

##### **B. Crear interacciones entre features**  
```python
# Ejemplo: Interacción entre bedrooms y bathrooms
df_optimized['bed_bath_interaction'] = df_optimized['bedrooms'] * df_optimized['bathrooms']

# Otras interacciones posibles
df_optimized['accommodates_per_bedroom'] = df_optimized['accommodates'] / (df_optimized['bedrooms'] + 1)  # +1 para evitar división por 0
```

##### **C. Codificación de variables categóricas**  
Si hay columnas categóricas no codificadas (ej: `neighbourhood`), usa **One-Hot Encoding** o **Target Encoding**.  

---

#### **3. Reducción de Columnas (Ejemplo para `property_type`)**  
Si hay muchas columnas dummy de `property_type` (ej: 50+), agrupa las menos frecuentes:  


---

### 📈 **Resultados Esperados**  
- **R² test mejorado** (ej: 0.75–0.80).  
- **MAE reducido** (ej: 20–22 en test).  
- **Modelo más interpretable** (menos features irrelevantes).  

### 🎯 **Conclusión**  
- **Si `price` tiene cola larga**, la transformación logarítmica es clave.  
- **Interacciones como `bedrooms × bathrooms`** capturan relaciones no lineales.  
- **Agrupar categorías raras** simplifica el modelo sin perder información.

### 📌 **Código Completo con Todas las Mejoras**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

# --- 0. Copia del DataFrame y limpieza ---
df_optimized = df_analysis.copy()
columns_to_drop = ['luxury_score', 'price_per_person', 'neighbourhood_price_avg', 'log_price']
df_optimized = df_optimized.drop(columns=columns_to_drop)


# Aplicar log1p si hay cola larga
use_log = True  # Cambiar a False si la distribución es normal
if use_log:
    y = np.log1p(df_optimized['price'])
else:
    y = df_optimized['price']

# --- 2. Ingeniería de features ---
# Interacción entre bedrooms y bathrooms
df_optimized['bed_bath_interaction'] = df_optimized['bedrooms'] * df_optimized['bathrooms']

# Agrupar categorías raras en 'property_type' (ejemplo)
property_type_columns = [col for col in df_optimized.columns if col.startswith('property_type_')]
if len(property_type_columns) > 0:
    property_counts = df_optimized[property_type_columns].sum()
    rare_properties = property_counts[property_counts < 50].index  # Menos de 50 muestras
    df_optimized['property_type_Other'] = df_optimized[rare_properties].sum(axis=1)
    df_optimized = df_optimized.drop(columns=rare_properties)

# --- 3. Definir X e y ---
X = df_optimized.drop(columns=['price'])
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# --- 4. Búsqueda de parámetros ---
param_grid_v3 = {
    'max_depth': [10, 15],
    'min_samples_leaf': [3, 5],
    'n_estimators': [100, 150],
    'max_features': ['sqrt', 0.3]
}

grid_search_v3 = GridSearchCV(
    RandomForestRegressor(random_state=42),
    param_grid_v3,
    cv=5,
    scoring='r2',
    n_jobs=-1
)
grid_search_v3.fit(X_train, y_train)

# --- 5. Evaluación ---
best_model_v3 = grid_search_v3.best_estimator_
y_pred_train_v3 = best_model_v3.predict(X_train)
y_pred_test_v3 = best_model_v3.predict(X_test)

# Revertir transformación logarítmica si se usó
if use_log:
    y_train_exp = np.expm1(y_train)
    y_test_exp = np.expm1(y_test)
    y_pred_train_exp = np.expm1(y_pred_train_v3)
    y_pred_test_exp = np.expm1(y_pred_test_v3)
else:
    y_train_exp, y_test_exp = y_train, y_test
    y_pred_train_exp, y_pred_test_exp = y_pred_train_v3, y_pred_test_v3

# Métricas
def print_metrics(y_true, y_pred, dataset_name=""):
    r2 = r2_score(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)

    print(f"\n📊 Métricas para {dataset_name}")
    print(f"R²:    {r2:.4f}")
    print(f"MSE:   {mse:.2f}")
    print(f"RMSE:  {rmse:.2f}")
    print(f"MAE:   {mae:.2f}")

print("✅ Mejores parámetros (v3):", grid_search_v3.best_params_)
print_metrics(y_train_exp, y_pred_train_exp, "Entrenamiento (v3)")
print_metrics(y_test_exp, y_pred_test_exp, "Prueba (v3)")

# Feature Importances
importances = best_model_v3.feature_importances_
feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': importances})
print("\n🔝 Top 10 Features más importantes (v3):")
print(feature_importance_df.sort_values(by='Importance', ascending=False).head(10))

"""### 📌 **Código Final Optimizado (v4)**
**Incorpora:**  
- Transformación logarítmica de `price` (confirmada por el histograma).  
- Escalado robusto de features numéricas.  
- Agrupamiento de categorías raras.  
- Hiperparámetros ajustados para generalización.  
- Métricas en escala original (EUR).  

---

### 📈 **Qué Esperar con Este Código**
1. **Mejor R² en test** (objetivo: **0.75+** vs 0.69 anterior).  
2. **MAE reducido** (objetivo: **<20 USD** vs 24 anterior).  
3. **Modelo más robusto** gracias a:  
   - Transformación logarítmica.  
   - Escalado robusto.  
   - Interacciones de features.  

---

### 🔍 **Si los resultados no mejoran**
1. **Verificar leakage**:  
   ```python
   print("Columnas potencialmente problemáticas:", [col for col in X.columns if "price" in col.lower()])
   ```
2. **Probar XGBoost**:  
   ```python
   from xgboost import XGBRegressor
   xgb_model = XGBRegressor(tree_method='gpu_hist', random_state=42)
   xgb_model.fit(X_train, y_train)
   ```

---

### 📉 **Nota Final**  
La transformación logarítmica es clave para manejar la cola larga de precios. Este código optimiza el equilibrio entre **precisión** y **generalización**.
"""

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.preprocessing import RobustScaler

# --- 0. Copia del DataFrame y limpieza ---
df_optimized = df_analysis.copy()
columns_to_drop = ['luxury_score', 'price_per_person', 'neighbourhood_price_avg', 'log_price']
df_optimized = df_optimized.drop(columns=columns_to_drop)

# --- 1. Transformación logarítmica de 'price' (confirmada por histograma) ---
y = np.log1p(df_optimized['price'])  # Transformación obligatoria por cola larga

# --- 2. Ingeniería de features ---
# Interacciones clave
df_optimized['bed_bath_ratio'] = df_optimized['bathrooms'] / (df_optimized['bedrooms'] + 1e-6)
df_optimized['acc_bed_interaction'] = df_optimized['accommodates'] * df_optimized['bedrooms']

# Agrupar categorías raras en 'property_type'
property_type_cols = [col for col in df_optimized.columns if col.startswith('property_type_')]
if len(property_type_cols) > 0:
    rare_properties = df_optimized[property_type_cols].sum()[df_optimized[property_type_cols].sum() < 20].index
    df_optimized['property_type_Other'] = df_optimized[rare_properties].sum(axis=1)
    df_optimized = df_optimized.drop(columns=rare_properties)

# --- 3. Escalado de features numéricas ---
numeric_features = ['accommodates', 'bathrooms', 'bedrooms', 'beds', 'minimum_nights', 'number_of_reviews']
scaler = RobustScaler()
df_optimized[numeric_features] = scaler.fit_transform(df_optimized[numeric_features])

# --- 4. Definir X e y ---
X = df_optimized.drop(columns=['price'])
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# --- 5. Búsqueda de hiperparámetros optimizados ---
param_grid_v4 = {
    'max_depth': [15, 20, None],      # Profundidad flexible
    'min_samples_leaf': [2, 3],       # Control de overfitting
    'n_estimators': [200, 300],       # Más árboles para estabilidad
    'max_features': ['sqrt', 0.5],    # Balance entre features
    'max_samples': [0.8, None]        # Submuestreo para diversidad
}

grid_search_v4 = GridSearchCV(
    RandomForestRegressor(random_state=42),
    param_grid_v4,
    cv=5,
    scoring='r2',
    n_jobs=-1
)
grid_search_v4.fit(X_train, y_train)

# --- 6. Evaluación del mejor modelo ---
best_model_v4 = grid_search_v4.best_estimator_
y_pred_train = best_model_v4.predict(X_train)
y_pred_test = best_model_v4.predict(X_test)

# Revertir transformación logarítmica para métricas
y_train_exp = np.expm1(y_train)
y_test_exp = np.expm1(y_test)
y_pred_train_exp = np.expm1(y_pred_train)
y_pred_test_exp = np.expm1(y_pred_test)

# --- 7. Métricas en escala original (EUR) ---
def print_metrics(y_true, y_pred, dataset_name):
    r2 = r2_score(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    print(f"\n📊 **Métricas para {dataset_name} (EUR)**")
    print(f"R²:    {r2:.4f} | MSE:   {mse:.2f}")
    print(f"RMSE:  {rmse:.2f} | MAE:   {mae:.2f}")

print("✅ **Mejores parámetros (v4):**", grid_search_v4.best_params_)
print_metrics(y_train_exp, y_pred_train_exp, "Entrenamiento")
print_metrics(y_test_exp, y_pred_test_exp, "Prueba")

# --- 8. Feature Importances ---
importances = best_model_v4.feature_importances_
top_features = pd.DataFrame({'Feature': X.columns, 'Importance': importances}) \
    .sort_values(by='Importance', ascending=False).head(10)
print("\n🔝 **Top 10 Features más importantes:**")
print(top_features.to_markdown(tablefmt="grid", index=False))

"""### 📌 **Código Final Optimizado (Random Forest v5)**
**Mejoras clave:**  
- Feature engineering avanzado (años de experiencia del host, reviews por mes).  
- Optimización de tiempo de ejecución (timeout ajustado).  
- Hiperparámetros afinados.
"""

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.preprocessing import RobustScaler

# --- 0. Copia del DataFrame y limpieza ---
# Cargar el DataFrame de análisis
from pathlib import Path
processed_data_dir = Path("../data/processed/")
df_analysis = pd.read_csv(processed_data_dir / "df_analysis.csv")
df_optimized = df_analysis.copy()
columns_to_drop = ['luxury_score', 'price_per_person', 'neighbourhood_price_avg', 'log_price']
df_optimized = df_optimized.drop(columns=columns_to_drop)
# Volcar df_optimized a CSV
# Asegúrate de que el directorio existe
processed_data_dir = Path("../data/processed/")
processed_data_dir.mkdir(parents=True, exist_ok=True)
df_optimized.to_csv(processed_data_dir / "df_optimized.csv", index=False)
print(f"✓ Saved to {processed_data_dir / 'df_optimized.csv'}")
print("="*80)

# --- 1. Transformación logarítmica de 'price' ---
y = np.log1p(df_optimized['price'])

# --- 2. Ingeniería de features avanzada ---
# Interacciones
df_optimized['bed_bath_ratio'] = df_optimized['bathrooms'] / (df_optimized['bedrooms'] + 1e-6)
df_optimized['acc_bed_interaction'] = df_optimized['accommodates'] * df_optimized['bedrooms']

# Nuevas features propuestas
df_optimized['host_experience_years'] = 2023 - df_optimized['host_since_year']
df_optimized['reviews_per_month'] = df_optimized['number_of_reviews'] / 12  # Ajustar si hay columna 'months_active'

# Agrupar categorías raras
property_type_cols = [col for col in df_optimized.columns if col.startswith('property_type_')]
if len(property_type_cols) > 0:
    rare_properties = df_optimized[property_type_cols].sum()[df_optimized[property_type_cols].sum() < 20].index
    df_optimized['property_type_Other'] = df_optimized[rare_properties].sum(axis=1)
    df_optimized = df_optimized.drop(columns=rare_properties)

# --- 3. Escalado robusto ---
numeric_features = ['accommodates', 'bathrooms', 'bedrooms', 'beds', 'minimum_nights', 'number_of_reviews', 'host_experience_years']
scaler = RobustScaler()
df_optimized[numeric_features] = scaler.fit_transform(df_optimized[numeric_features])

# --- 4. Definir X e y ---
X = df_optimized.drop(columns=['price', 'host_since_year'])  # Eliminar columna original de año
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# --- 5. Búsqueda de hiperparámetros (optimizada para tiempo) ---
param_grid_v5 = {
    'max_depth': [None, 20],
    'min_samples_leaf': [1, 2],
    'n_estimators': [300],
    'max_features': [0.5],
    'max_samples': [0.8]
}

grid_search_v5 = GridSearchCV(
    RandomForestRegressor(random_state=42),
    param_grid_v5,
    cv=5,
    scoring='r2',
    n_jobs=4  # Reducir workers para evitar timeout
)
grid_search_v5.fit(X_train, y_train)

# --- 6. Evaluación ---
best_model_v5 = grid_search_v5.best_estimator_
y_pred_train = best_model_v5.predict(X_train)
y_pred_test = best_model_v5.predict(X_test)

# Revertir transformación logarítmica
y_train_exp, y_test_exp = np.expm1(y_train), np.expm1(y_test)
y_pred_train_exp, y_pred_test_exp = np.expm1(y_pred_train), np.expm1(y_pred_test)

# --- 7. Métricas ---
def print_metrics(y_true, y_pred, dataset_name):
    r2 = r2_score(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    print(f"\n📊 **Métricas para {dataset_name} (USD)**")
    print(f"R²:    {r2:.4f} | MSE:   {mse:.2f}")
    print(f"RMSE:  {rmse:.2f} | MAE:   {mae:.2f}")

print("✅ **Mejores parámetros (v5):**", grid_search_v5.best_params_)
print_metrics(y_train_exp, y_pred_train_exp, "Entrenamiento")
print_metrics(y_test_exp, y_pred_test_exp, "Prueba")

# --- 8. Feature Importances ---
importances = best_model_v5.feature_importances_
top_features = pd.DataFrame({'Feature': X.columns, 'Importance': importances}) \
    .sort_values(by='Importance', ascending=False).head(10)
print("\n🔝 **Top 10 Features más importantes (v5):**")
print(top_features.to_markdown(tablefmt="grid", index=False))

"""---

### 🚀 **Código para XGBoost (Comparación Directa)**
**Ventajas:**  
- Mayor velocidad de entrenamiento.  
- Potencial mejor R² con tuning adecuado.  

---

### 📊 **Comparativa Esperada**
| Modelo          | R² Test (Esperado) | MAE Test (Esperado) | Tiempo Entrenamiento |
|----------------|--------------------|---------------------|----------------------|
| Random Forest  | 0.77 - 0.79        | 19 - 21             | Alto (~10-15 min)    |
| XGBoost        | **0.78 - 0.82**    | **18 - 20**         | Bajo (~2-5 min)      |

**Recomendación:**  
- Usa **Random Forest** si priorizas interpretabilidad (importancia de features).  
- Usa **XGBoost** si buscas máxima precisión y velocidad.  

Ambos códigos incluyen:  
- Transformación logarítmica reversible.  
- Métricas en USD.  
- Identificación de features clave.
"""

import xgboost as xgb
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import numpy as np

# --- 1. Preparar datos ---
# (Usar mismo X_train/X_test que en Random Forest)
# Asegurar que todas las variables categóricas están codificadas

# --- 2. Entrenamiento con hiperparámetros base (CPU) ---
xgb_model = xgb.XGBRegressor(
    objective='reg:squarederror',
    n_estimators=300,
    max_depth=6,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.5,
    random_state=42,
    tree_method='hist'  # Cambiado a CPU (hist/histogram)
)
xgb_model.fit(X_train, y_train)

# --- 3. Predicciones ---
y_pred_train_xgb = xgb_model.predict(X_train)
y_pred_test_xgb = xgb_model.predict(X_test)

# Revertir transformación logarítmica
y_pred_train_xgb_exp = np.expm1(y_pred_train_xgb)
y_pred_test_xgb_exp = np.expm1(y_pred_test_xgb)

# --- 4. Métricas ---
def print_metrics(y_true, y_pred, dataset_name):
    r2 = r2_score(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    print(f"\n📊 **Métricas para {dataset_name} (USD)**")
    print(f"R²:    {r2:.4f} | MSE:   {mse:.2f}")
    print(f"RMSE:  {rmse:.2f} | MAE:   {mae:.2f}")

print("\n⚡ **Métricas para XGBoost (Base)**")
print_metrics(y_train_exp, y_pred_train_xgb_exp, "Entrenamiento")
print_metrics(y_test_exp, y_pred_test_xgb_exp, "Prueba")

# --- 5. Feature Importances ---
xgb_importances = xgb_model.feature_importances_
top_features_xgb = pd.DataFrame({'Feature': X.columns, 'Importance': xgb_importances}) \
    .sort_values(by='Importance', ascending=False).head(10)
print("\n🔝 **Top 10 Features (XGBoost Base):**")
print(top_features_xgb.to_markdown(tablefmt="grid", index=False))

# --- 6. Optimización con GridSearch (Opcional) ---
param_grid_xgb = {
    'max_depth': [4, 6, 8],
    'learning_rate': [0.05, 0.1],
    'n_estimators': [200, 300],
    'subsample': [0.8, 1.0]
}

grid_search_xgb = GridSearchCV(
    xgb.XGBRegressor(objective='reg:squarederror', random_state=42, tree_method='hist'),
    param_grid_xgb,
    cv=5,
    scoring='r2',
    n_jobs=4
)
grid_search_xgb.fit(X_train, y_train)

# --- 7. Métricas del modelo optimizado ---
print("\n✅ **Mejores parámetros (XGBoost Optimizado):**", grid_search_xgb.best_params_)
y_pred_train_xgb_opt = grid_search_xgb.predict(X_train)
y_pred_test_xgb_opt = grid_search_xgb.predict(X_test)
y_pred_train_xgb_opt_exp = np.expm1(y_pred_train_xgb_opt)
y_pred_test_xgb_opt_exp = np.expm1(y_pred_test_xgb_opt)

print_metrics(y_train_exp, y_pred_train_xgb_opt_exp, "Entrenamiento (Optimizado)")
print_metrics(y_test_exp, y_pred_test_xgb_opt_exp, "Prueba (Optimizado)")

# --- 8. Feature Importances (Optimizado) ---
xgb_importances_opt = grid_search_xgb.best_estimator_.feature_importances_
top_features_xgb_opt = pd.DataFrame({'Feature': X.columns, 'Importance': xgb_importances_opt}) \
    .sort_values(by='Importance', ascending=False).head(10)
print("\n🔝 **Top 10 Features (XGBoost Optimizado):**")
print(top_features_xgb_opt.to_markdown(tablefmt="grid", index=False))

"""### 🔍 **Análisis de Resultados: XGBoost vs Random Forest**

#### 📊 **Comparativa de Métricas**
| Modelo               | R² Train | R² Test  | MAE Test | Tiempo Estimado |
|----------------------|----------|----------|----------|-----------------|
| **Random Forest v5** | 0.89     | **0.76** | 20.35    | ~15 min         |
| **XGBoost (Base)**   | 0.71     | 0.67     | 25.13    | ~3 min          |
| **XGBoost (Optimizado)** | 0.81 | **0.72** | 22.77    | ~10 min         |

#### 🎯 **Conclusiones Clave**
1. **Random Forest sigue siendo mejor**:
   - Mayor R² en test (0.76 vs 0.72 de XGBoost optimizado).
   - Menor MAE (20.35 vs 22.77).
   - Más equilibrado (diferencia train-test: 13% vs 9% en XGBoost optimizado).

2. **Problema con XGBoost**:
   - **Overfitting en la versión base** (R² train 0.71 vs test 0.67).
   - **Importancia de features desbalanceada** (`room_type_Entire_home_apt` domina con 86% en optimizado).

3. **¿Por qué dos resultados en XGBoost?**:
   - **Primer bloque**: Modelo base con parámetros por defecto.
   - **Segundo bloque**: Modelo optimizado con GridSearch (mejores hiperparámetros).

---

### 🚀 **Recomendación Final**
**Quédate con Random Forest** porque:  
✅ **Mayor precisión** en test (R² 0.76 vs 0.72).  
✅ **Errores más bajos** (MAE 20.35 vs 22.77).  
✅ **Interpretabilidad** (importancia de features más balanceada).  

**Usa XGBoost solo si:**  
➡ Necesitas velocidad (es más rápido con datasets grandes).  
➡ Quieres experimentar con otros parámetros (ej: reducir `max_depth` a 4-6).  

---

### 📌 **Pasos para Mejorar XGBoost (Opcional)**
Si decides seguir con XGBoost, ajusta estos parámetros:
```python
xgb_model_v2 = xgb.XGBRegressor(
    objective='reg:squarederror',
    n_estimators=200,
    max_depth=4,  # Reducir profundidad
    learning_rate=0.05,  # Tasa de aprendizaje más baja
    subsample=0.7,
    colsample_bytree=0.3,  # Menos features por árbol
    random_state=42,
    tree_method='hist'
)
```
**Objetivo:** Reducir overfitting y balancear importancia de features.

---

### 🔥 **Código Definitivo (Random Forest v5)**
```python
# Usa el código de Random Forest v5 anterior (el que te dio R² test = 0.76)
# Es tu mejor modelo actualmente.
```

**Nota:** Si el tiempo de entrenamiento de Random Forest es un problema, considera:
- Reducir `n_estimators` a 200.
- Usar `max_samples=0.7` para acelerar.  

¡El modelo está listo para producción! 🎉

### 🔥 **Código Optimizado (XGBoost v2)**
"""

import xgboost as xgb
from sklearn.metrics import r2_score, root_mean_squared_error, mean_absolute_error
import numpy as np

# --- 1. Preparar datos ---
# (Usar mismo X_train/X_test que en Random Forest)
# Asegurar que todas las variables categóricas están codificadas

# --- 2. Entrenamiento con hiperparámetros base (CPU) ---
xgb_model_v2 = xgb.XGBRegressor(
    objective='reg:squarederror',
    n_estimators=200,
    max_depth=4,  # Reducir profundidad
    learning_rate=0.05,  # Tasa de aprendizaje más baja
    subsample=0.7,
    colsample_bytree=0.3,  # Menos features por árbol
    random_state=42,
    tree_method='hist'
)
xgb_model_v2.fit(X_train, y_train)

# --- 3. Predicciones ---
y_pred_train_xgb = xgb_model_v2.predict(X_train)
y_pred_test_xgb = xgb_model_v2.predict(X_test)

# Revertir transformación logarítmica
y_pred_train_xgb_exp = np.expm1(y_pred_train_xgb)
y_pred_test_xgb_exp = np.expm1(y_pred_test_xgb)

# --- 4. Métricas ---
def print_metrics(y_true, y_pred, dataset_name):
    r2 = r2_score(y_true, y_pred)
    mse = root_mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    print(f"\n📊 **Métricas para {dataset_name} (EUR)**")
    print(f"R²:    {r2:.4f} | MSE:   {mse:.2f}")
    print(f"RMSE:  {rmse:.2f} | MAE:   {mae:.2f}")

print("\n⚡ **Métricas para XGBoost (Base)**")
print_metrics(y_train_exp, y_pred_train_xgb_exp, "Entrenamiento")
print_metrics(y_test_exp, y_pred_test_xgb_exp, "Prueba")

# --- 5. Feature Importances ---
xgb_importances = xgb_model_v2.feature_importances_
top_features_xgb = pd.DataFrame({'Feature': X.columns, 'Importance': xgb_importances}) \
    .sort_values(by='Importance', ascending=False).head(10)
print("\n🔝 **Top 10 Features (XGBoost Base):**")
print(top_features_xgb.to_markdown(tablefmt="grid", index=False))

# --- 6. Optimización con GridSearch (Opcional) ---
param_grid_xgb = {
    'max_depth': [4, 6, 8],
    'learning_rate': [0.05, 0.1],
    'n_estimators': [200, 300],
    'subsample': [0.8, 1.0]
}

grid_search_xgb = GridSearchCV(
    xgb.XGBRegressor(objective='reg:squarederror', random_state=42, tree_method='hist'),
    param_grid_xgb,
    cv=5,
    scoring='r2',
    n_jobs=4
)
grid_search_xgb.fit(X_train, y_train)

# --- 7. Métricas del modelo optimizado ---
print("\n✅ **Mejores parámetros (XGBoost Optimizado):**", grid_search_xgb.best_params_)
y_pred_train_xgb_opt = grid_search_xgb.predict(X_train)
y_pred_test_xgb_opt = grid_search_xgb.predict(X_test)
y_pred_train_xgb_opt_exp = np.expm1(y_pred_train_xgb_opt)
y_pred_test_xgb_opt_exp = np.expm1(y_pred_test_xgb_opt)

print_metrics(y_train_exp, y_pred_train_xgb_opt_exp, "Entrenamiento (Optimizado)")
print_metrics(y_test_exp, y_pred_test_xgb_opt_exp, "Prueba (Optimizado)")

# --- 8. Feature Importances (Optimizado) ---
xgb_importances_opt = grid_search_xgb.best_estimator_.feature_importances_
top_features_xgb_opt = pd.DataFrame({'Feature': X.columns, 'Importance': xgb_importances_opt}) \
    .sort_values(by='Importance', ascending=False).head(10)
print("\n🔝 **Top 10 Features (XGBoost Optimizado):**")
print(top_features_xgb_opt.to_markdown(tablefmt="grid", index=False))

"""### 🚀 **Random Forest v6 - Código Optimizado para Mejorar R² Test (0.77 → 0.80+)**

Versión mejorada con **feature engineering estratégico** y **ajuste de hiperparámetros** para maximizar el R² en test, manteniendo la estructura de tus códigos anteriores:

---

### 📌 **Mejoras Clave Respecto a v5**
1. **Nuevas Features Interactivas**:
   - `bed_bath_ratio`: Captura la relación entre baños y habitaciones.
   - `acc_to_beds`: Ratio de capacidad vs camas disponibles.
   - `reviews_per_month`: Dinámica de actividad del listado.

2. **Ajuste de Hiperparámetros**:
   - `max_features=0.33` (en lugar de 0.5) → Mayor generalización.
   - `min_samples_leaf=3` → Reduce overfitting.
   - `n_estimators=300` → Más estabilidad.

3. **Escalado Robusto**:
   - Aplicado solo a features numéricas críticas (evita distorsión en categóricas).

4. **Métricas en EUR**:
   - Todas las métricas se reportan en escala original (tras revertir `log1p`).

---

### 📈 **Resultados Esperados**
| Métrica       | v5 (Anterior) | v6 (Esperado) |
|--------------|---------------|---------------|
| **R² Test**  | 0.76          | **0.78-0.80** |
| **MAE Test** | 20.35         | **18-19**     |

---

### 🔍 **¿Por Qué Funciona Mejor?**
- **Interacciones no lineales**: Las nuevas features capturan relaciones complejas entre variables.
- **Control de overfitting**: Parámetros más restrictivos (`max_features=0.33`, `min_samples_leaf=3`).
- **Escalado inteligente**: RobustScaler protege contra outliers sin afectar relaciones no lineales.

---

### 🚨 **Si el R² No Mejora**
1. **Verifica fugas de datos**:
   ```python
   print([col for col in X.columns if 'price' in col.lower()])
   ```
2. **Prueba reducir más `max_features`** (ej: 0.25).
3. **Añade más datos** si es posible (el tamaño de muestra afecta directamente al R²).
"""

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.preprocessing import RobustScaler

# --- 0. Cargar datos y selección de features ---
df_reduced = df_analysis.copy()

# Lista de features relevantes (sin fugas)
features_relevantes = [
    'accommodates', 'bathrooms', 'bedrooms', 'beds', 'minimum_nights',
    'number_of_reviews', 'review_scores_rating', 'host_is_superhost',
    'host_since_year', 'neighbourhood_density', 'host_experience',
    'has_wifi', 'has_air_conditioning', 'has_pool', 'has_kitchen', 'has_washer'
]

# --- 1. Ingeniería de features AVANZADA ---
# Interacciones clave
df_reduced['bed_bath_ratio'] = df_reduced['bathrooms'] / (df_reduced['bedrooms'] + 1e-6)
df_reduced['acc_to_beds'] = df_reduced['accommodates'] / (df_reduced['beds'] + 1e-6)
df_reduced['reviews_per_month'] = df_reduced['number_of_reviews'] / 12  # Asumiendo 1 año de antigüedad mínima

# Experiencia del host (mejorada)
# Calculate host experience years and drop 'host_since_year' only after all operations are completed
df_reduced['host_experience_years'] = 2023 - df_reduced['host_since_year']

# Ensure 'host_since_year' is dropped only after all operations requiring it are done
if 'host_since_year' in df_reduced.columns:
    df_reduced.drop(columns=['host_since_year'], inplace=True)

# --- 2. Transformación logarítmica del target ---
y = np.log1p(df_reduced['price'])  # Para manejar cola larga
# Cargar el archivo CSV para obtener la columna necesaria
processed_data_dir = Path("../data/processed/")
df_analysis_path = processed_data_dir / "df_analysis.csv"
df_analysis_full = pd.read_csv(df_analysis_path)

# Asegurarse de que la columna 'host_since_year' esté presente
if 'host_since_year' in df_analysis_full.columns:
    df_reduced['host_since_year'] = df_analysis_full['host_since_year']
else:
    raise KeyError("La columna 'host_since_year' no está presente en el archivo df_analysis.csv")

# Seleccionar las columnas relevantes
X = df_reduced[features_relevantes + ['bed_bath_ratio', 'acc_to_beds', 'reviews_per_month', 'host_experience_years']]

# --- 3. Escalado robusto de numéricas ---
numeric_features = ['accommodates', 'bathrooms', 'bedrooms', 'beds', 'minimum_nights',
                   'number_of_reviews', 'neighbourhood_density', 'host_experience_years']
scaler = RobustScaler()
X[numeric_features] = scaler.fit_transform(X[numeric_features])

# --- 4. División train-test ---
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# --- 5. Modelo RandomForest OPTIMIZADO (v6) ---
model_v6 = RandomForestRegressor(
    n_estimators=300,          # Más árboles para estabilidad
    max_depth=None,            # Profundidad ilimitada (controlada por min_samples_leaf)
    min_samples_leaf=3,        # Reducir overfitting
    max_features=0.33,         # Menos features por árbol (mejor generalización)
    max_samples=0.8,           # Submuestreo para diversidad
    random_state=42,
    n_jobs=-1
)
model_v6.fit(X_train, y_train)

# --- 6. Predicciones y métricas ---
def print_metrics(y_true, y_pred, dataset_name):
    y_true_exp = np.expm1(y_true)
    y_pred_exp = np.expm1(y_pred)
    r2 = r2_score(y_true_exp, y_pred_exp)
    mae = mean_absolute_error(y_true_exp, y_pred_exp)
    print(f"\n📊 **{dataset_name}**")
    print(f"R²: {r2:.4f} | MAE: {mae:.2f} USD")

y_pred_train = model_v6.predict(X_train)
y_pred_test = model_v6.predict(X_test)

print_metrics(y_train, y_pred_train, "ENTRENAMIENTO (v6)")
print_metrics(y_test, y_pred_test, "PRUEBA (v6)")

# --- 7. Análisis de features ---
importances = model_v6.feature_importances_
top_features = pd.DataFrame({'Feature': X.columns, 'Importance': importances}) \
    .sort_values(by='Importance', ascending=False).head(10)
print("\n🔝 **Top 10 Features (v6):**")
print(top_features.to_markdown(tablefmt="grid"))

"""### 🔍 **Análisis de Resultados (v6)**
Los resultados muestran:
- **R² Train: 0.80** (bueno, pero podría ser mejor)
- **R² Test: 0.69** (ligera mejora vs XGBoost, pero por debajo de tu v5)
- **Overfitting**: Diferencia del 11% entre train/test (aceptable pero mejorable)

---

### 🚀 **Estrategias para Mejorar el Modelo (v7)**

#### 1. **Reducir Overfitting**
- **Aumentar `min_samples_leaf`**: De 3 a 5 para mayor generalización.
- **Limitar `max_depth`**: Probar 15-20 en lugar de `None`.
- **Reducir `n_estimators`**: 200 en lugar de 300 (menos árboles = menos varianza).

#### 2. **Mejorar Feature Engineering**
- **Agregar interacción clave**: `review_scores_rating * number_of_reviews` (calidad × popularidad).
- **Transformar `minimum_nights`**: Aplicar `np.log1p` si tiene cola larga.
- **Codificar `host_is_superhost` como numérico (0/1)** si no lo está.

#### 3. **Ajustar Hiperparámetros**
```python
param_grid_v7 = {
    'max_depth': [15, 20],          # Limitar profundidad
    'min_samples_leaf': [3, 5],     # Más riguroso
    'n_estimators': [200, 250],     # Equilibrio velocidad/performance
    'max_features': [0.3, 0.4],     # Más restricción
    'max_samples': [0.7, 0.8]       # Mayor submuestreo
}
```

---

### 📈 **Resultados Esperados (v7)**
| Métrica       | v6 (Actual) | v7 (Objetivo) |
|--------------|-------------|---------------|
| **R² Test**  | 0.69        | **0.73-0.75** |
| **MAE Test** | 24.39       | **<22**       |

---

### 🔍 **Diagnóstico Adicional**
Si el R² no mejora:
1. **Verifica correlaciones**:
   ```python
   print(X.corrwith(np.expm1(y)).sort_values(ascending=False))
   ```
2. **Prueba eliminar features poco importantes** (importancia < 0.01).
3. **Considera técnicas avanzadas**:
   - **Stacking**: Combina RandomForest con XGBoost.
   - **Embeddings categóricos**: Para `neighbourhood_density`.

---

### 🎯 **Conclusión**
El código v7 está optimizado para **maximizar el R² en test** sin overfitting. Si tras ejecutarlo no alcanzas el 0.75, sería recomendable:
1. Revisar leakage de datos (¿alguna variable contiene info de `price`?).  
2. Aumentar el tamaño del dataset (si es posible).  
3. Probar modelos alternativos (Gradient Boosting o redes neuronales).

### 📌 **Código Optimizado (Random Forest v7)**
"""

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import r2_score, mean_absolute_error
from sklearn.preprocessing import RobustScaler

# --- 0. Preparación de datos ---
df_reduced = df_analysis.copy()

# Feature engineering mejorado
df_reduced['bed_bath_ratio'] = df_reduced['bathrooms'] / (df_reduced['bedrooms'] + 1e-6)
df_reduced['acc_to_beds'] = df_reduced['accommodates'] / (df_reduced['beds'] + 1e-6)
df_reduced['reviews_per_month'] = df_reduced['number_of_reviews'] / 12
df_reduced['rating_popularity'] = df_reduced['review_scores_rating'] * np.log1p(df_reduced['number_of_reviews'])
df_reduced['host_experience_years'] = 2023 - df_reduced['host_since_year']
df_reduced['minimum_nights_log'] = np.log1p(df_reduced['minimum_nights'])

# Selección final de features
features_v7 = [
    'accommodates', 'bathrooms', 'bedrooms', 'beds', 'minimum_nights_log',
    'number_of_reviews', 'review_scores_rating', 'host_is_superhost',
    'neighbourhood_density', 'host_experience_years', 'has_wifi',
    'has_air_conditioning', 'bed_bath_ratio', 'acc_to_beds',
    'reviews_per_month', 'rating_popularity'
]

X = df_reduced[features_v7]
y = np.log1p(df_reduced['price'])

# Escalado robusto
numeric_features = ['accommodates', 'bathrooms', 'bedrooms', 'beds', 'minimum_nights_log',
                   'number_of_reviews', 'neighbourhood_density', 'host_experience_years']
scaler = RobustScaler()
X[numeric_features] = scaler.fit_transform(X[numeric_features])

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# --- Modelo y GridSearch ---
param_grid_v7 = {
    'max_depth': [15, 20],
    'min_samples_leaf': [3, 5],
    'n_estimators': [200, 250],
    'max_features': [0.3, 0.4],
    'max_samples': [0.7, 0.8]
}

grid_search_v7 = GridSearchCV(
    RandomForestRegressor(random_state=42, n_jobs=-1),
    param_grid_v7,
    cv=5,
    scoring='r2'
)
grid_search_v7.fit(X_train, y_train)

# --- Evaluación ---
best_model_v7 = grid_search_v7.best_estimator_
y_pred_test = best_model_v7.predict(X_test)

def print_metrics(y_true, y_pred):
    y_true_exp = np.expm1(y_true)
    y_pred_exp = np.expm1(y_pred)
    r2 = r2_score(y_true_exp, y_pred_exp)
    mae = mean_absolute_error(y_true_exp, y_pred_exp)
    print(f"R² Test: {r2:.4f} | MAE Test: {mae:.2f} EUR")

print("✅ Mejores parámetros:", grid_search_v7.best_params_)
print_metrics(y_test, y_pred_test)

"""----------------"""