# -*- coding: utf-8 -*-
"""inside_airbnb_eda_3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KRhRe3dzSHb5lh_C_reVKLgjMI0jvLum

### **Paso 1: Unificaci√≥n y Detecci√≥n de Duplicados**
**Objetivo**: Combinar los 5 archivos CSV en un √∫nico DataFrame y verificar si hay duplicados entre los listings.

#### **Acciones a realizar**:
1. **Cargar los archivos CSV**:
"""

# -*- coding: utf-8 -*-
import pandas as pd
from pathlib import Path

# Configurar pandas para mostrar todas las salidas
pd.set_option('display.max_columns', None)  # Mostrar todas las columnas
pd.set_option('display.max_rows', None)     # Mostrar todas las filas
pd.set_option('display.width', 1000)        # Ancho m√°ximo del display
pd.set_option('display.max_colwidth', None) # Mostrar contenido completo de columnas

raw_data_dir = Path("../data/raw/inside/")
processed_data_dir = Path("../data/processed/")
files = [
    raw_data_dir / "listings-03-2024.csv",
    raw_data_dir / "listings-03-2025.csv",
    raw_data_dir / "listings-06-2024.csv",
    raw_data_dir / "listings-12-2024.csv",
]

# Leer y concatenar archivos
print("="*80)
print("1. READING AND CONCATENATING ALL FILES...")
dfs = [pd.read_csv(file) for file in files]
df = pd.concat(dfs, ignore_index=True)
print("‚úì Files combined successfully")
print("="*80)


# Funci√≥n para mostrar secciones claramente
def show_section(title, content, max_lines=20):
    print("\n" + "="*80)
    print(f"{title.upper()}")
    print("="*80)
    if isinstance(content, (pd.DataFrame, pd.Series)):
        with pd.option_context('display.max_rows', max_lines):
            print(content)
    else:
        print(content)

# Mostrar informaci√≥n b√°sica
show_section("3. BASIC DATAFRAME INFORMATION", df.info())

# Mostrar filas
show_section("4. FIRST 5 ROWS", df.head())
show_section("5. LAST 5 ROWS", df.tail())

# Estad√≠sticas
show_section("6. DESCRIPTIVE STATISTICS", df.describe(include='all'))

# Uso de memoria
show_section("7. MEMORY USAGE", df.memory_usage(deep=True))

# Tipos de datos
show_section("8. DATA TYPES", df.dtypes)

# Columnas y forma
show_section("9. COLUMN NAMES", df.columns.tolist())
show_section("10. DATAFRAME SHAPE", f"Rows: {df.shape[0]}, Columns: {df.shape[1]}")

# Valores √∫nicos
show_section("11. UNIQUE VALUES COUNT PER COLUMN", df.nunique())

# Valores nulos
show_section("12. NULL VALUES COUNT PER COLUMN", df.isnull().sum())

# Duplicados
show_section("13. DUPLICATED ROWS COUNT", df.duplicated().sum())

# Valores √∫nicos por columna (detallado)
show_section("14. DETAILED UNIQUE VALUES PER COLUMN", "\n".join([f"{col}: {df[col].nunique()}" for col in df.columns]))

# Valores √∫nicos por fila (primeras 5)
show_section("15. UNIQUE VALUES PER ROW (FIRST 5 ROWS)", "\n".join([f"Row {i}: {df.iloc[i].nunique()}" for i in range(min(5, df.shape[0]))]))

print("\n" + "="*80)
print("‚úì ALL DATA HAS BEEN DISPLAYED")
print("="*80)

"""2. **Verificar duplicados**:
   - Usaremos `id` (identificador √∫nico de Airbnb) y `scrape_id` (fecha de scraping) para detectar si un mismo listing aparece en m√∫ltiples archivos.
"""

# Verificar duplicados basados en 'id' y 'scrape_id'
duplicates = df.duplicated(subset=['id', 'scrape_id'], keep=False)
print(f"N√∫mero de filas duplicadas: {duplicates.sum()}")

"""3. **Eliminar duplicados** (si los hay):"""

df = df.drop_duplicates(subset=['id', 'scrape_id'], keep='last')

"""4. **Guardar el DataFrame unificado** (opcional, para no repetir el proceso):"""

# Volcar el dataframe combinado a un archivo CSV
print("="*80)
print("1. SAVING COMBINED DATA TO CSV...")
raw_data_dir.mkdir(parents=True, exist_ok=True)
print(f"‚úì Created directory: {raw_data_dir}")
df.to_csv(raw_data_dir / "raw_combined_listings.csv", index=False)
print(f"‚úì Saved to {raw_data_dir / 'raw_combined_listings.csv'}")

"""#### **Qu√© esperamos analizar**:
- ¬øHay listings que aparecen en m√∫ltiples archivos? (ej. si un mismo `id` tiene diferentes `scrape_id`).
- ¬øLos duplicados son exactos o hay diferencias en columnas como `price`, `number_of_reviews`, etc.?

---

### **Siguiente paso (una vez me compartas los resultados)**:
- An√°lisis de valores nulos y columnas irrelevantes (eliminaremos las que no aporten al modelo de predicci√≥n de precios).
- Estad√≠sticas descriptivas b√°sicas de las columnas num√©ricas (como `price`, `bedrooms`, etc.).

---

**Por favor, comparte**:
1. El n√∫mero total de filas despu√©s de unificar los archivos.
2. El n√∫mero de duplicados encontrados (si hubo).
3. ¬øAlguna observaci√≥n interesante? (ej. ¬ølos duplicados tienen precios diferentes?).

As√≠ ajustamos el siguiente paso basado en estos resultados. ¬°Vamos poco a poco!

### **An√°lisis de los Resultados y Siguiente Paso**

#### **Observaciones Clave:**
1. **Datos Unificados Correctamente**:  
   - Se han combinado **104,996 registros** de los 5 archivos CSV sin duplicados exactos (`duplicates.sum() = 0`).  
   - No hay filas duplicadas (mismo `id` y `scrape_id`), pero hay listings con el mismo `id` en diferentes archivos (ej: actualizaciones temporales).  
---

2. **Problemas Detectados**:  
   - **Columnas con Alta Cardinalidad**:  
     - `amenities` (90,997 valores √∫nicos), `description` (38,189 √∫nicos), `name` (38,541 √∫nicos). Dif√≠cil de usar directamente como predictor.  
     - `price` est√° como texto (ej: `"$31.00"`) y tiene valores nulos (8,2397 no nulos).  
   - **Valores Nulos Relevantes**:  
     - `price` (21.5% nulos), `bathrooms` (21.5% nulos), `bedrooms` (9.3% nulos), `beds` (21.6% nulos).  
     - Variables clave como `review_scores_rating` (20.8% nulos) y `host_response_rate` (19.7% nulos).  

     3. **Columnas con Baja Utilidad**:  
   - URLs (`listing_url`, `picture_url`), IDs (`id`, `scrape_id`), columnas temporales (`last_scraped`, `host_since`).  
   - Columnas redundantes: `host_listings_count` vs `calculated_host_listings_count`.

### **Siguiente Paso: Limpieza Inicial y Preparaci√≥n de Variables**  
**Objetivo**: Preparar el dataset para el EDA enfoc√°ndonos en predictores relevantes para el precio.

#### **Acciones Propuestas**:  
1. **Eliminar Columnas No Relevantess**:
"""

columns_to_drop = [
    'id', 'listing_url', 'scrape_id', 'last_scraped', 'source', 'host_url',
    'host_thumbnail_url', 'host_picture_url', 'calendar_updated',
    'license', 'calendar_last_scraped', 'neighbourhood'  # Redundante con neighbourhood_cleansed
]
df_clean = df.drop(columns=columns_to_drop)

"""2. **Convertir `price` a Num√©rico**:"""

df_clean['price'] = df_clean['price'].str.replace('$', '').str.replace(',', '').astype(float)
print(f"Price column converted to float. Sample values:\n{df_clean['price'].head()}")

"""3. **Manejar Valores Nulos en Columnas Clave**:  
   - Eliminar filas donde `price` es nulo (es nuestra variable objetivo).  
   - Imputar nulos en `bedrooms`, `bathrooms`, y `beds` con la mediana por `room_type`:
"""

for col in ['bedrooms', 'bathrooms', 'beds']:
    df_clean[col] = df_clean.groupby('room_type')[col].transform(lambda x: x.fillna(x.median()))

"""
4. **Crear Variables Derivadas**:  
   - Extraer el a√±o del host (`host_since`) para usarlo como antig√ºedad:  """

df_clean['host_since_year'] = pd.to_datetime(df_clean['host_since']).dt.year

"""5. **Filtrar Columnas para An√°lisis Inicial**:  
   - Seleccionar predictores potenciales basados en relevancia para el precio:
"""

predictors = [
    'neighbourhood_cleansed', 'room_type', 'accommodates', 'bathrooms', 'bedrooms', 'beds',
    'minimum_nights', 'number_of_reviews', 'review_scores_rating', 'instant_bookable',
    'host_is_superhost', 'host_since_year', 'property_type'
]
df_analysis = df_clean[['price'] + predictors]
# Mostrar informaci√≥n del dataframe para an√°lisis
print(f"Dataframe for analysis created with {len(df_analysis)} rows and {len(df_analysis.columns)} columns.")
# Mostrar las primeras filas del dataframe para an√°lisis
print(f"Sample data:\n{df_analysis.head()}")
# Volcar a CSV
df_analysis.to_csv(processed_data_dir / "processed_listings.csv", index=False)
print(f"‚úì Saved to {processed_data_dir / 'processed_listings.csv'}")
print("="*80)
print("‚úì DATA PREPROCESSING COMPLETED")

"""#### **Qu√© Esperamos**:  
- Un dataset limpio con `price` como variable num√©rica y predictores listos para an√°lisis.  
- Reducci√≥n de ruido al eliminar columnas irrelevantes.  

---

### **Siguiente Paso: An√°lisis de Correlaciones y Feature Engineering**  
**Objetivo**: Identificar predictores fuertes, transformar variables y manejar outliers.

#### **1. An√°lisis Inicial de Correlaci√≥n**
"""

import seaborn as sns
import matplotlib.pyplot as plt

# Matriz de correlaci√≥n (solo num√©ricas)
corr_matrix = df_analysis.corr(numeric_only=True)
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix[['price']].sort_values(by='price', ascending=False), annot=True, cmap='coolwarm')
plt.title("Correlaci√≥n con Precio")
plt.show()

"""**Qu√© buscamos**:  
- Variables num√©ricas con correlaci√≥n alta (`accommodates`, `bedrooms`, `bathrooms`).  
- Variables categ√≥ricas prometedoras (`room_type`, `neighbourhood_cleansed`).

#### **2. Transformaciones Clave**  
**a) Codificar Variables Categ√≥ricas**:
"""

# One-Hot Encoding para 'room_type' y 'property_type' (ej: Entire home/apt vs Private room)
df_analysis = pd.get_dummies(df_analysis, columns=['room_type', 'property_type'], drop_first=True)

"""**b) Crear Features Derivadas**:  
- **Densidad de Listings por Barrio**:
"""

neighbourhood_density = df_analysis['neighbourhood_cleansed'].value_counts(normalize=True)
df_analysis['neighbourhood_density'] = df_analysis['neighbourhood_cleansed'].map(neighbourhood_density)

"""- **Antig√ºedad del Host (a√±os)**:"""

df_analysis['host_experience'] = 2025 - df_analysis['host_since_year']  # Asumiendo a√±o actual 2025

"""**c) Tratar Outliers en `price`**:  """

# Eliminar precios extremos (ej: > percentil 99)
price_upper_limit = df_analysis['price'].quantile(0.99)
df_analysis = df_analysis[df_analysis['price'] <= price_upper_limit]

"""#### **3. An√°lisis de Distribuciones**  """

import numpy as np

# Distribuci√≥n de 'price' (log-scale para normalizar)
plt.figure(figsize=(10, 6))
sns.histplot(np.log1p(df_analysis['price']), kde=True)
plt.title("Distribuci√≥n de Precios (log)")
plt.show()

"""**Acci√≥n**: Aplicar log-transform a `price` si la distribuci√≥n es sesgada.

#### **4. Manejar Texto en `amenities` (Opcional pero Potente)**
"""

# Ejemplo: Crear dummy para amenidades clave (WiFi, Aire Acondicionado)
df_analysis['has_wifi'] = df_clean['amenities'].str.contains('Wifi', case=False).astype(int)
df_analysis['has_air_conditioning'] = df_clean['amenities'].str.contains('Air conditioning', case=False).astype(int)

"""### **Resultado Esperado**  
Un dataframe con:  
- Variables num√©ricas limpias (`price`, `accommodates`, etc.).  
- Categ√≥ricas codificadas (`room_type_Entire home/apt`).  
- Features derivadas (`neighbourhood_density`, `host_experience`).  
- Outliers controlados.  

---

### **Pr√≥ximos Pasos**  
1. **Ejecutar el c√≥digo anterior**:  
   - Gr√°ficos de correlaci√≥n/distribuci√≥n.  
   - % de nulos restantes.  
2. **Decidir juntos**:  
   - ¬øIncluir `amenities` como dummies? (Aumentar√° dimensionalidad).  
   - ¬øTransformar m√°s variables (ej: discretizar `minimum_nights`)?

### **An√°lisis de los Resultados y Siguientes Acciones**

#### **1. Hallazgos Clave**  
- **Correlaciones Bajas**:  
  - Las variables num√©ricas tienen correlaciones d√©biles con `price` (la m√°s alta es `accommodates` con **0.18**).  
  - `review_scores_rating` y `host_experience` casi no impactan (correlaci√≥n < 0.03).  
  - Variables como `number_of_reviews` y `minimum_nights` muestran correlaci√≥n negativa insignificante.  

- **Distribuci√≥n de Precios**:  
  - La distribuci√≥n de `price` (tras log-transform) sigue una curva cercana a la normal, pero con colas largas (outliers residuales).  

- **Amenidades**:  
  - Features como `has_wifi` o `has_air_conditioning` podr√≠an a√±adir se√±al, pero requieren an√°lisis adicional.  

---

#### **2. Problemas Identificados**  
- **Predictores D√©biles**: Las variables actuales no explican bien el precio (R¬≤ bajo esperable).  
- **Falta de Contexto Geogr√°fico**: `neighbourhood_cleansed` es categ√≥rica y no se ha explotado.  
- **Amenidades No Cuantificadas**: Solo se probaron 2 dummies, pero hay m√°s informaci√≥n en el texto.  

---

### **Siguientes Pasos para Mejorar el Modelo**

#### **1. Feature Engineering Avanzado**  
**a) Codificar `neighbourhood_cleansed` con Target Encoding**:  
   - Reemplazar el barrio por el **precio promedio** hist√≥rico en esa zona (evita alta dimensionalidad vs One-Hot).
"""

neighbourhood_price = df_analysis.groupby('neighbourhood_cleansed')['price'].mean().to_dict()
df_analysis['neighbourhood_price_avg'] = df_analysis['neighbourhood_cleansed'].map(neighbourhood_price)

"""**b) Crear `avg_score_per_amenity`**:  
   - Calificar amenidades por su "premium" asociado (ej: piscina aumenta un X% el precio).  

"""

amenities_list = ['Pool', 'Air conditioning', 'Wifi', 'Kitchen', 'Washer']
for amenity in amenities_list:
    df_analysis[f'has_{amenity.lower().replace(" ", "_")}'] = df_clean['amenities'].str.contains(amenity, case=False).astype(int)

"""**c) Discretizar `minimum_nights`**:  
   - Convertir en categor√≠as: `short_stay` (<7 noches), `medium_stay` (7-30), `long_stay` (>30).
"""

bins = [0, 7, 30, np.inf]
labels = ['short_stay', 'medium_stay', 'long_stay']
df_analysis['stay_type'] = pd.cut(df_analysis['minimum_nights'], bins=bins, labels=labels)
df_analysis = pd.get_dummies(df_analysis, columns=['stay_type'], drop_first=True)

"""---
#### **2. Reducci√≥n de Outliers**  
**a) Eliminar el 1% Extremo en `price`**:  
"""

lower = df_analysis['price'].quantile(0.01)
upper = df_analysis['price'].quantile(0.99)
df_analysis = df_analysis[(df_analysis['price'] >= lower) & (df_analysis['price'] <= upper)]

"""**b) Transformaci√≥n Logar√≠tmica de `price`**:"""

df_analysis['log_price'] = np.log1p(df_analysis['price'])

"""#### **3. An√°lisis Visual para Validar Relaciones**  
**a) Boxplot de `price` por `room_type`**:

--------------------------------------------------------------------------------
**Problema con el boxplot de room_type_Entire_apt**
"""

df_analysis.head()

# Mostrar las columnas del dataframe de an√°lisis
print(f"Dataframe for analysis created with {len(df_analysis)} rows and {len(df_analysis.columns)} columns.")

# Imprimir nombres de las columnas
print(f"Column names: {df_analysis.columns.tolist()}")

# Comprobamos si df_clean tiene la columna 'room_type'
print(f"Does df_clean have 'room_type' column? {'room_type' in df_clean.columns}")

# A√±adimos la columna 'room_type' al dataframe de an√°lisis
df_analysis['room_type'] = df_clean['room_type']  # Asumiendo que df_clean tiene la columna original

# Comprobamos nueva columna a√±adida 'room_type' a df_analysis
df_analysis.head()

# Lista de columnas dummy de room_type
room_dummies = ['room_type_Hotel room', 'room_type_Private room', 'room_type_Shared room']

# Crear columna para Entire home/apt (1 si todas las dem√°s son 0)
df_analysis['room_type_Entire_home_apt'] = (df_analysis[room_dummies].sum(axis=1) == 0).astype(int)

df_analysis.head()

"""**Solucionado**

---
"""

sns.boxplot(x='room_type_Entire_home_apt', y='price', data=df_analysis)
plt.title("Precio por Tipo de Alojamiento")
plt.show()

"""**b) Scatterplot de `accommodates` vs `price`**:  """

sns.scatterplot(x='accommodates', y='price', hue='neighbourhood_price_avg', data=df_analysis)
plt.title("Capacidad vs Precio (Color por Barrio)")
plt.show()

"""### **Resultado Esperado**  
Un dataframe con:  
- **Variables m√°s informativas**:  
  - `neighbourhood_price_avg` (geograf√≠a como se√±al num√©rica).  
  - Amenidades clave como dummies (`has_pool`, `has_air_conditioning`).  
  - `stay_type_medium_stay` y `stay_type_long_stay` (impacto de estancia m√≠nima).  
- **Target (`price`) normalizado** y sin outliers extremos.

---

### **Pr√≥ximos Pasos**  
1. **Ejecuta el c√≥digo de transformaci√≥n** y comparte:  
   - Nuevas correlaciones (¬ømejor√≥ la se√±al de `neighbourhood_price_avg`?).  
   - Gr√°ficos de `room_type` y `accommodates`.  
2. **Decidir juntos**:  
   - ¬øIncluir interacciones (ej: `accommodates * room_type`)?  
   - ¬øProbamos modelos b√°sicos (Linear Regression, Random Forest) para ver importancia de features?

### **An√°lisis de las Gr√°ficas Generadas**

#### **1. Boxplot: Precio por Tipo de Alojamiento**  
- **Interpretaci√≥n**:  
  - Los listings de tipo `Entire home/apt` (valor `1`) tienen una mediana de precio claramente m√°s alta que otros tipos (`Private room`, `Shared room`, etc.).  
  - Hay outliers en ambos grupos, especialmente en `Entire home/apt`, lo que sugiere la presencia de propiedades de lujo o ubicaciones premium.  

- **Acci√≥n Recomendada**:  
  - **Mantener esta variable** en el modelo: es un predictor fuerte (como ya sospech√°bamos).  
  - **Tratar outliers**: Aplicar un l√≠mite superior (ej: percentil 95) para evitar que distorsionen el modelo.
"""

# Ejemplo: Filtrar outliers de precio
price_upper_limit = df_analysis['price'].quantile(0.95)
df_filtered = df_analysis[df_analysis['price'] <= price_upper_limit]

"""---

#### **2. Scatterplot: Capacidad (`accommodates`) vs Precio (Color por Barrio)**  
- **Interpretaci√≥n**:  
  - Relaci√≥n positiva entre `accommodates` y `price`, pero no lineal (ej: propiedades para 6+ personas pueden tener precios similares a las de 4-5).  
  - Los colores (barrios) muestran que la ubicaci√≥n tambi√©n impacta: algunos barrios tienen precios consistentemente m√°s altos independientemente de la capacidad.

- **Acci√≥n Recomendada**:  
  - **Crear interacciones**: Combinar `accommodates` con `neighbourhood_price_avg` para capturar c√≥mo el precio por persona var√≠a por zona.  
  - **Discretizar capacidad**: Agrupar en categor√≠as como `small` (1-2), `medium` (3-5), `large` (6+).
"""

# Interacci√≥n entre accommodates y barrio
df_analysis['price_per_person'] = df_analysis['price'] / df_analysis['accommodates']

# Discretizar accommodates
df_analysis['accommodates_group'] = pd.cut(
    df_analysis['accommodates'],
    bins=[0, 2, 5, np.inf],
    labels=['small', 'medium', 'large']
)

"""---

### **Pr√≥ximos Pasos para Mejorar el Modelo**

#### **1. Feature Engineering Adicional**  
- **Amenidades Premium**:  
  Usar las columnas `has_pool`, `has_air_conditioning`, etc., para crear un **"score de lujo"** (suma de amenidades premium).
"""

amenities_premium = ['has_pool', 'has_air_conditioning', 'has_washer']
df_analysis['luxury_score'] = df_analysis[amenities_premium].sum(axis=1)

"""- **Antig√ºedad del Host**:  
  Transformar `host_since_year` en categor√≠as (ej: `new_host` (<2 a√±os), `experienced_host` (2-5), `veteran_host` (>5)).

#### **2. Correlaciones Actualizadas**  
Ejecuta una nueva matriz de correlaci√≥n incluyendo las nuevas variables (`price_per_person`, `luxury_score`, etc.):
"""

corr_matrix = df_analysis.corr(numeric_only=True)
plt.figure(figsize=(8, 8))
sns.heatmap(corr_matrix[['price']].sort_values(by='price', ascending=False), annot=True, cmap='coolwarm')

"""**Soluci√≥n al Error**:

Transformaci√≥n de neighbourhood_cleansed
El error ocurre porque RandomForestRegressor no puede manejar directamente variables categ√≥ricas como strings (ej: 'C√°rmenes'). Necesitamos codificar la columna neighbourhood_cleansed en un formato num√©rico. Aqu√≠ tienes las opciones:

**Opci√≥n 1**:

**Target Encoding** (Recomendado para barrios)
Reemplaza cada barrio por el precio promedio hist√≥rico en esa zona. Esto captura la relaci√≥n entre ubicaci√≥n y precio sin a√±adir alta dimensionalidad.
"""

# Calcular precio promedio por barrio
neighbourhood_avg_price = df_analysis.groupby('neighbourhood_cleansed')['price'].mean().to_dict()

# Crear nueva columna num√©rica
df_analysis['neighbourhood_encoded'] = df_analysis['neighbourhood_cleansed'].map(neighbourhood_avg_price)

# Eliminar la columna original de strings
df_analysis.drop(columns=['neighbourhood_cleansed'], inplace=True)

df_analysis.head()

"""### **Soluci√≥n al Error: Conversi√≥n de Variables Categ√≥ricas/Strings a Num√©ricas**

El error ocurre porque `RandomForestRegressor` no puede manejar directamente columnas con strings (`'f'`, `'t'`) o booleanos (`True`, `False`). Necesitamos convertir **todas las columnas no num√©ricas** a formato num√©rico. Aqu√≠ est√° c√≥mo hacerlo:

---

### **1. Identificar Columnas Problem√°ticas**
Primero, verifica qu√© columnas no son num√©ricas:
"""

print(df_analysis.dtypes)

"""- Las columnas con `object`, `bool` o strings (ej: `'f'`, `'t'`) deben transformarse.

***

### **Soluci√≥n al Error: Manejo de Valores `NaN` en Columnas Booleanas**

El error ocurre porque algunas de tus columnas booleanas contienen valores `NaN` (nulos), y al intentar convertirlas directamente a enteros (`int`), Python no sabe c√≥mo manejar estos valores nulos. Aqu√≠ te muestro c√≥mo solucionarlo:
"""

# Sacar las columnas boleanas
bool_columns = df_analysis.select_dtypes(include=['bool']).columns.tolist()
print(f"Boolean columns: {bool_columns}")

"""---

### **Paso 1: Identificar Columnas con Valores Nulos**
Primero, verifica qu√© columnas tienen valores nulos:
"""

print(df_analysis[bool_columns].isnull().sum())

"""---

### **Paso 2: Estrategia para Manejar `NaN`**
Tienes dos opciones para manejar los valores nulos en columnas booleanas:

#### **Opci√≥n A: Rellenar `NaN` con un Valor por Defecto (ej: 0)**
"""

for col in bool_columns:
    if col in df_analysis.columns:
        # Rellenar NaN con 0 y luego convertir
        df_analysis[col] = (
            df_analysis[col]
            .fillna(0)  # Rellenar NaN con 0 (o 1 si prefieres)
            .astype(str)
            .replace({'t': 1, 'f': 0, 'True': 1, 'False': 0})
            .astype(int)
        )

df_analysis.head()

"""#### **Opci√≥n B: Eliminar Filas con `NaN` en Columnas Booleanas**"""

# df_analysis.dropna(subset=bool_columns, inplace=True)

"""---

### **Paso 3: Convertir Columnas Booleanas (`True`/`False`) a `1`/`0`**
Las columnas como `stay_type_medium_stay` y `stay_type_long_stay` son de tipo `bool` (no `object`), as√≠ que puedes convertirlas directamente:
"""

bool_cols_non_object = ['stay_type_medium_stay', 'stay_type_long_stay'] + \
                      [col for col in df_analysis.columns if df_analysis[col].dtype == bool]

for col in bool_cols_non_object:
    if col in df_analysis.columns:
        df_analysis[col] = df_analysis[col].astype(int)

"""---

### **Paso 4: Eliminar Columnas Redundantes**
Elimina la columna `room_type` (ya tienes las dummies):
"""

df_analysis.drop(columns=['room_type'], inplace=True, errors='ignore')

"""### **Paso 5: Verificar Tipos de Datos Finales**"""

print(df_analysis.dtypes)

"""- Aseg√∫rate de que todas las columnas sean `int64`, `float64`, o `category`."""

df_analysis['host_is_superhost'] = df_analysis['host_is_superhost'].fillna('f')

df_analysis['host_is_superhost'] = df_analysis['host_is_superhost'].replace({'t': 1, 'f': 0}).astype(int)

df_analysis['host_is_superhost'] = (
    df_analysis['host_is_superhost']
    .replace({'t': 1, 'f': 0})
    .infer_objects(copy=False)
    .astype(int)
)

# instant_bookable

df_analysis['instant_bookable'] = (
    df_analysis['instant_bookable']
    .replace({'t': 1, 'f': 0})
    .infer_objects(copy=False)
    .astype(int)
)

print(df_analysis['host_is_superhost'].unique())

print(df_analysis['accommodates_group'].unique())

df_analysis = pd.get_dummies(
    df_analysis,
    columns=['accommodates_group'],
    prefix='acc_group',
    dtype=int  # Esto asegura que obtienes 1 y 0
)

# Elimina una columna para evitar multicolinealidad
df_analysis.drop(columns=['acc_group_small'], inplace=True)

# Imprimir las primeras filas del dataframe de an√°lisis
print(f"Dataframe for analysis created with {len(df_analysis)} rows and {len(df_analysis.columns)} columns.")
# Imprimir las primeras filas del dataframe de an√°lisis
print(f"Sample data:\n{df_analysis.head()}")

# Contar las columnas totales
total_columns = len(df_analysis.columns)
print(f"Total columns in df_analysis: {total_columns}")
# recoger todas las columnas
all_columns = df_analysis.columns.tolist()
print(f"All columns in df_analysis: {all_columns}")

# Volcar df_analysis a CSV
# Aseg√∫rate de que el directorio existe
processed_data_dir = Path("../data/processed/")
processed_data_dir.mkdir(parents=True, exist_ok=True)
df_analysis.to_csv(processed_data_dir / "df_analysis.csv", index=False)
print(f"‚úì Saved to {processed_data_dir / 'df_analysis.csv'}")
print("="*80)
print("‚úì DATA PREPROCESSING COMPLETED")

"""---

### **Comprobaci√≥n del dataframe con MODELOS**
---

"""

# from sklearn.ensemble import RandomForestRegressor

# X = df_analysis.drop(columns=['price', 'log_price'])  # Features
# y = df_analysis['price']  # Target

# model = RandomForestRegressor(n_estimators=100, random_state=42)
# model.fit(X, y)

"""---

### **Notas Adicionales**
1. **Si persisten errores**:  
   - Verifica que no queden columnas no num√©ricas con `print(df_analysis.dtypes)`.  
   - Si hay columnas `category` (como `accommodates_group`), convi√©rtelas a dummies:

2. **Estrategia para `NaN`**:  
   - Si decides rellenar con `0`, aseg√∫rate de que no distorsione el an√°lisis (ej: `NaN` en `host_is_superhost` podr√≠a interpretarse como "no superhost").  

3. **Columnas con muchos `NaN`**:  
   - Si una columna tiene muchos `NaN`, considera eliminarla o imputar valores basados en otras variables.

---

### **Ejemplo de DataFrame Corregido**
| price | accommodates | ... | instant_bookable (1/0) | host_is_superhost (1/0) | ... | stay_type_medium_stay (1/0) |
|-------|--------------|-----|------------------------|--------------------------|-----|-----------------------------|
| 100   | 2            | ... | 1                      | 0                        | ... | 1                           |

¬°Con estos cambios, tu modelo deber√≠a ejecutarse correctamente! Si necesitas m√°s ajustes, dime qu√© columnas espec√≠ficas a√∫n dan problemas.

### **4. Verificar que Todas las Columnas sean Num√©ricas**
"""

print(df_analysis.dtypes)

"""- Aseg√∫rate de que **todas** las columnas sean `int64`, `float64`, etc. Si alguna no lo es, repite los pasos anteriores.

---

### **Notas Adicionales**
- **`neighbourhood_encoded`**: Si usaste Target Encoding para los barrios, aseg√∫rate de que sea `float64`.  
- **Columnas con pocos valores √∫nicos**: Si hay columnas como `accommodates_group` (`small`, `medium`, `large`), apl√≠cales `pd.get_dummies()` o `LabelEncoder`.  
- **Valores nulos**: Si el modelo a√∫n falla, verifica nulos con `df_analysis.isnull().sum()` y elim√≠nalos o imputa valores.

---

### **Ejemplo de DataFrame Listo para Modelar**
| price | accommodates | bathrooms | ... | instant_bookable (1/0) | host_is_superhost (1/0) | ... |
|-------|--------------|-----------|-----|------------------------|--------------------------|-----|
| 100   | 2            | 1.0       | ... | 1                      | 0                        | ... |

---

### **¬øPor qu√© Funciona Esto?**
- Los modelos de sklearn requieren que **todas las features sean num√©ricas**.  
- Las conversiones a `1`/`0` o valores continuos (Target Encoding) mantienen la informaci√≥n sin perder significado.  

Si el error persiste, comparte el resultado de `df_analysis.dtypes` para revisar columnas espec√≠ficas.
"""

# from sklearn.ensemble import RandomForestRegressor

# # Separar features y target
# X = df_analysis.drop(columns=['price', 'log_price'])  # Excluir target
# y = df_analysis['price']

# # Entrenar modelo
# model = RandomForestRegressor(n_estimators=100, random_state=42)
# model.fit(X, y)

# # Importancia de features
# pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False).head(10)

"""---

### **Resumen de Acciones**  
1. **Filtrar outliers** de precio (percentil 95).  
2. **Crear interacciones**: `price_per_person` y `accommodates_group`.  
3. **Generar nuevas features**: `luxury_score` y categor√≠as de antig√ºedad del host.  
4. **Actualizar correlaciones** y probar un modelo inicial.  

¬øQuieres que profundicemos en alguno de estos pasos? Por ejemplo, ¬øprefieres priorizar el an√°lisis de outliers o el feature engineering?
"""

# Correlaci√≥n de todas las variables con 'price'
correlations = df_analysis.corr()['price'].abs().sort_values(ascending=False)
print(correlations.head(10))

"""### **Comprobamos que todas las columnas del dataframe `df_analysis` sean num√©ricas**"""

# Comprobamos las columnas del dataframe de an√°lisis
print(f"Dataframe for analysis created with {len(df_analysis)} rows and {len(df_analysis.columns)} columns.")
# Imprimir nombres de las columnas
print(f"Columnas en df_analysis: {df_analysis.columns.tolist()}")
# Imprimir las primeras filas del dataframe para an√°lisis
df_analysis.head()

"""# **ENTRENAMIENTO Y VALIDACI√ìN DE MODELOS**"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import numpy as np

# --- 0. Preparaci√≥n de los datos ---
# Separar caracter√≠sticas y objetivo
# 1. Crear copia para trabajar (seguridad)
df_leakage = df_analysis.copy()

# 2. Eliminar columnas problem√°ticas
columns_to_drop = ['luxury_score', 'price_per_person', 'neighbourhood_price_avg', 'log_price']
df_leakage = df_leakage.drop(columns=columns_to_drop)

# 3. Guardar DataFrame limpio (opcional, pero buena pr√°ctica)
df_leakage.to_csv('../data/processed/data_leakage.csv', index=False)

# 4. Definir X e y
X = df_leakage.drop(columns='price')
y = df_leakage['price']

# --- 1. Definici√≥n de la grilla de par√°metros ---
param_grid_optimized = {
    'max_depth': [10, 15, 20],
    'min_samples_leaf': [2, 3, 4],
    'n_estimators': [100, 200],
    'max_features': ['sqrt', 0.5]
}

# --- 2. Divisi√≥n de los datos ---
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# --- 3. Grid Search con validaci√≥n cruzada ---
grid_search_optimized = GridSearchCV(
    RandomForestRegressor(random_state=42),
    param_grid_optimized,
    cv=5,
    scoring='r2',
    n_jobs=-1
)
grid_search_optimized.fit(X_train, y_train)

# --- 4. Mejor modelo encontrado ---
best_model_optimized = grid_search_optimized.best_estimator_

# --- 5. Predicciones ---
y_pred_train = best_model_optimized.predict(X_train)
y_pred_test = best_model_optimized.predict(X_test)

# --- 6. Funci√≥n para mostrar m√©tricas ---
def print_metrics(y_true, y_pred, dataset_name=""):
    r2 = r2_score(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)

    print(f"\nüìä M√©tricas para {dataset_name}")
    print(f"R¬≤:    {r2:.4f}")
    print(f"MSE:   {mse:.2f}")
    print(f"RMSE:  {rmse:.2f}")
    print(f"MAE:   {mae:.2f}")

# --- 7. Resultados finales ---
print("‚úÖ Mejores par√°metros encontrados:", grid_search_optimized.best_params_)

print_metrics(y_train, y_pred_train, "Entrenamiento")
print_metrics(y_test, y_pred_test, "Prueba")

"""### üìä **An√°lisis de tus Resultados Actuales**
- **R¬≤ en entrenamiento (0.895) vs prueba (0.763):**  
  - Hay un **sobreajuste moderado** (diferencia del ~13% entre train y test).  
  - El modelo generaliza decentemente, pero puede mejorar.  

- **Error absoluto (MAE):**  
  - En test, el error promedio es de **21.9 unidades monetarias**.  
  - Para un negocio como Airbnb, esto puede ser aceptable o no, dependiendo del rango de precios (ej: si el precio promedio es 100‚Ç¨, un MAE de 21.9 es alto).  

- **Consistencia del modelo:**  
  - Los par√°metros √≥ptimos tienen `max_depth=20` y `n_estimators=200`, lo que sugiere un modelo complejo. Podr√≠a simplificarse para reducir overfitting.  

---

### üöÄ **Estrategias para Mejorar el Modelo**
#### **1. Reducir el Sobreajuste**  
- **Aumentar `min_samples_leaf`:** Fuerza a que cada hoja del √°rbol tenga m√°s muestras, reduciendo complejidad.  
- **Limitar `max_depth`:** Prueba valores menores (ej: 10, 15).  
- **Usar `max_samples`:** Entrena cada √°rbol con un subconjunto aleatorio de datos (menos correlaci√≥n entre √°rboles).  

#### **2. Ingenier√≠a de Features**  
- **Agrupar categor√≠as poco frecuentes** en `property_type` o `room_type` (ej: "Other").  
- **Crear interacciones entre features** (ej: `bedrooms` √ó `bathrooms`).  

#### **3. Transformaci√≥n de Target**  
- Si `price` tiene cola larga, aplica `np.log1p(y)` para normalizar y mejorar el R¬≤.  

#### **4. Otros Modelos**  
- Prueba **Gradient Boosting (XGBoost, LightGBM)**, que suelen generalizar mejor.  

---


### üìå **Resultados Esperados**  
- **R¬≤ test m√°s cercano a train** (ej: 0.80‚Äì0.85 vs 0.89 en train).  
- **MAE reducido** (ej: 18‚Äì20 en test).  
- **Modelo m√°s robusto** (menos sensible a ruido).  

---

### üìà **Pasos Adicionales (si a√∫n hay margen de mejora)**  
1. **Prueba XGBoost:**  
   ```python
   from xgboost import XGBRegressor
   xgb_model = XGBRegressor(random_state=42, tree_method='hist')
   xgb_model.fit(X_train, y_train)
   y_pred_test_xgb = xgb_model.predict(X_test)
   print_metrics(y_test, y_pred_test_xgb, "Prueba (XGBoost)")
   ```

2. **Transformaci√≥n logar√≠tmica de `y`:**  
   ```python
   y_train_log = np.log1p(y_train)
   y_test_log = np.log1p(y_test)
   model_log = RandomForestRegressor(**grid_search_v2.best_params_)
   model_log.fit(X_train, y_train_log)
   y_pred_test_log = np.expm1(model_log.predict(X_test))  # Revertir transformaci√≥n
   print_metrics(y_test, y_pred_test_log, "Prueba (Log Transform)")
   ```

3. **Ensamblaje de modelos** (RandomForest + XGBoost).  

---

### üéØ **Conclusi√≥n**  
El modelo actual es decente, pero con **par√°metros m√°s restrictivos** y **transformaciones de datos**, puedes lograr:  
- **+5‚Äì10% en R¬≤ test**  
- **Errores absolutos (MAE) m√°s bajos**.

### üî• **C√≥digo Mejorado (con todas las validaciones)**
"""

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

# --- 0. Copia del DataFrame y limpieza ---
df_optimized = df_analysis.copy()
columns_to_drop = ['luxury_score', 'price_per_person', 'neighbourhood_price_avg', 'log_price']
df_optimized = df_optimized.drop(columns=columns_to_drop)

# --- 1. Definir X e y ---
X = df_optimized.drop(columns='price')
y = df_optimized['price']

# --- 2. Divisi√≥n train-test ---
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# --- 3. Nueva grilla de par√°metros (enfoque anti-overfitting) ---
param_grid_optimized_v2 = {
    'max_depth': [10, 15],  # Reducir profundidad m√°xima
    'min_samples_leaf': [3, 5],  # Aumentar muestras por hoja
    'n_estimators': [100, 150],  # Menos √°rboles
    'max_features': ['sqrt', 0.3],  # Menos features por √°rbol
    'max_samples': [0.8, None]  # Entrenar con 80% de datos por √°rbol
}

# --- 4. B√∫squeda con validaci√≥n cruzada ---
grid_search_v2 = GridSearchCV(
    RandomForestRegressor(random_state=42),
    param_grid_optimized_v2,
    cv=5,
    scoring='r2',
    n_jobs=-1
)
grid_search_v2.fit(X_train, y_train)

# --- 5. Mejor modelo y predicciones ---
best_model_v2 = grid_search_v2.best_estimator_
y_pred_train_v2 = best_model_v2.predict(X_train)
y_pred_test_v2 = best_model_v2.predict(X_test)

# --- 6. M√©tricas ---
def print_metrics(y_true, y_pred, dataset_name=""):
    r2 = r2_score(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)

    print(f"\nüìä M√©tricas para {dataset_name}")
    print(f"R¬≤:    {r2:.4f}")
    print(f"MSE:   {mse:.2f}")
    print(f"RMSE:  {rmse:.2f}")
    print(f"MAE:   {mae:.2f}")

print("‚úÖ Mejores par√°metros (v2):", grid_search_v2.best_params_)
print_metrics(y_train, y_pred_train_v2, "Entrenamiento (v2)")
print_metrics(y_test, y_pred_test_v2, "Prueba (v2)")

# --- 7. Feature Importances ---
importances = best_model_v2.feature_importances_
feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': importances})
print("\nüîù Top 10 Features m√°s importantes:")
print(feature_importance_df.sort_values(by='Importance', ascending=False).head(10))

"""### üîç **An√°lisis de Resultados Actuales**
Los resultados muestran:
- **R¬≤ test: 0.7185** (ligera mejora en generalizaci√≥n vs modelo anterior, pero a√∫n hay margen).  
- **MAE test: 24.63** (error absoluto alto para precios bajos/medios).  
- **Feature Importances**: `bedrooms`, `accommodates`, y `bathrooms` son las m√°s relevantes (coherente).  

---

### üõ† **Pasos para Mejorar el Modelo**

#### **1. Transformaci√≥n Logar√≠tmica del Target (`price`)**
**¬øPor qu√©?**  
Si `price` tiene una distribuci√≥n asim√©trica (cola larga), aplicar `log1p` puede normalizarla y mejorar el modelo.  


**Si la distribuci√≥n original est√° muy sesgada**, usa `y_log = np.log1p(y)` en el modelo.
"""

import matplotlib.pyplot as plt

# Histograma de 'price'
plt.figure(figsize=(10, 4))
plt.subplot(1, 2, 1)
plt.hist(y, bins=50, color='blue', alpha=0.7)
plt.title("Distribuci√≥n Original de 'price'")

# Histograma de 'log(price + 1)'
plt.subplot(1, 2, 2)
plt.hist(np.log1p(y), bins=50, color='green', alpha=0.7)
plt.title("Distribuci√≥n Logar√≠tmica de 'price'")
plt.show()

"""#### **2. Ingenier√≠a de Features**
##### **A. Agrupar categor√≠as poco frecuentes**  
**Ejemplo para `property_type`:**  
```python
# Contar frecuencias de cada categor√≠a
property_counts = df_optimized['property_type'].value_counts()

# Identificar categor√≠as con menos del 1% de los datos
rare_categories = property_counts[property_counts / len(df_optimized) < 0.01].index

# Agruparlas como "Other"
df_optimized['property_type'] = df_optimized['property_type'].replace(rare_categories, 'Other')
```

##### **B. Crear interacciones entre features**  
```python
# Ejemplo: Interacci√≥n entre bedrooms y bathrooms
df_optimized['bed_bath_interaction'] = df_optimized['bedrooms'] * df_optimized['bathrooms']

# Otras interacciones posibles
df_optimized['accommodates_per_bedroom'] = df_optimized['accommodates'] / (df_optimized['bedrooms'] + 1)  # +1 para evitar divisi√≥n por 0
```

##### **C. Codificaci√≥n de variables categ√≥ricas**  
Si hay columnas categ√≥ricas no codificadas (ej: `neighbourhood`), usa **One-Hot Encoding** o **Target Encoding**.  

---

#### **3. Reducci√≥n de Columnas (Ejemplo para `property_type`)**  
Si hay muchas columnas dummy de `property_type` (ej: 50+), agrupa las menos frecuentes:  


---

### üìà **Resultados Esperados**  
- **R¬≤ test mejorado** (ej: 0.75‚Äì0.80).  
- **MAE reducido** (ej: 20‚Äì22 en test).  
- **Modelo m√°s interpretable** (menos features irrelevantes).  

### üéØ **Conclusi√≥n**  
- **Si `price` tiene cola larga**, la transformaci√≥n logar√≠tmica es clave.  
- **Interacciones como `bedrooms √ó bathrooms`** capturan relaciones no lineales.  
- **Agrupar categor√≠as raras** simplifica el modelo sin perder informaci√≥n.

### üìå **C√≥digo Completo con Todas las Mejoras**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

# --- 0. Copia del DataFrame y limpieza ---
df_optimized = df_analysis.copy()
columns_to_drop = ['luxury_score', 'price_per_person', 'neighbourhood_price_avg', 'log_price']
df_optimized = df_optimized.drop(columns=columns_to_drop)


# Aplicar log1p si hay cola larga
use_log = True  # Cambiar a False si la distribuci√≥n es normal
if use_log:
    y = np.log1p(df_optimized['price'])
else:
    y = df_optimized['price']

# --- 2. Ingenier√≠a de features ---
# Interacci√≥n entre bedrooms y bathrooms
df_optimized['bed_bath_interaction'] = df_optimized['bedrooms'] * df_optimized['bathrooms']

# Agrupar categor√≠as raras en 'property_type' (ejemplo)
property_type_columns = [col for col in df_optimized.columns if col.startswith('property_type_')]
if len(property_type_columns) > 0:
    property_counts = df_optimized[property_type_columns].sum()
    rare_properties = property_counts[property_counts < 50].index  # Menos de 50 muestras
    df_optimized['property_type_Other'] = df_optimized[rare_properties].sum(axis=1)
    df_optimized = df_optimized.drop(columns=rare_properties)

# --- 3. Definir X e y ---
X = df_optimized.drop(columns=['price'])
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# --- 4. B√∫squeda de par√°metros ---
param_grid_v3 = {
    'max_depth': [10, 15],
    'min_samples_leaf': [3, 5],
    'n_estimators': [100, 150],
    'max_features': ['sqrt', 0.3]
}

grid_search_v3 = GridSearchCV(
    RandomForestRegressor(random_state=42),
    param_grid_v3,
    cv=5,
    scoring='r2',
    n_jobs=-1
)
grid_search_v3.fit(X_train, y_train)

# --- 5. Evaluaci√≥n ---
best_model_v3 = grid_search_v3.best_estimator_
y_pred_train_v3 = best_model_v3.predict(X_train)
y_pred_test_v3 = best_model_v3.predict(X_test)

# Revertir transformaci√≥n logar√≠tmica si se us√≥
if use_log:
    y_train_exp = np.expm1(y_train)
    y_test_exp = np.expm1(y_test)
    y_pred_train_exp = np.expm1(y_pred_train_v3)
    y_pred_test_exp = np.expm1(y_pred_test_v3)
else:
    y_train_exp, y_test_exp = y_train, y_test
    y_pred_train_exp, y_pred_test_exp = y_pred_train_v3, y_pred_test_v3

# M√©tricas
def print_metrics(y_true, y_pred, dataset_name=""):
    r2 = r2_score(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)

    print(f"\nüìä M√©tricas para {dataset_name}")
    print(f"R¬≤:    {r2:.4f}")
    print(f"MSE:   {mse:.2f}")
    print(f"RMSE:  {rmse:.2f}")
    print(f"MAE:   {mae:.2f}")

print("‚úÖ Mejores par√°metros (v3):", grid_search_v3.best_params_)
print_metrics(y_train_exp, y_pred_train_exp, "Entrenamiento (v3)")
print_metrics(y_test_exp, y_pred_test_exp, "Prueba (v3)")

# Feature Importances
importances = best_model_v3.feature_importances_
feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': importances})
print("\nüîù Top 10 Features m√°s importantes (v3):")
print(feature_importance_df.sort_values(by='Importance', ascending=False).head(10))

"""### üìå **C√≥digo Final Optimizado (v4)**
**Incorpora:**  
- Transformaci√≥n logar√≠tmica de `price` (confirmada por el histograma).  
- Escalado robusto de features num√©ricas.  
- Agrupamiento de categor√≠as raras.  
- Hiperpar√°metros ajustados para generalizaci√≥n.  
- M√©tricas en escala original (EUR).  

---

### üìà **Qu√© Esperar con Este C√≥digo**
1. **Mejor R¬≤ en test** (objetivo: **0.75+** vs 0.69 anterior).  
2. **MAE reducido** (objetivo: **<20 USD** vs 24 anterior).  
3. **Modelo m√°s robusto** gracias a:  
   - Transformaci√≥n logar√≠tmica.  
   - Escalado robusto.  
   - Interacciones de features.  

---

### üîç **Si los resultados no mejoran**
1. **Verificar leakage**:  
   ```python
   print("Columnas potencialmente problem√°ticas:", [col for col in X.columns if "price" in col.lower()])
   ```
2. **Probar XGBoost**:  
   ```python
   from xgboost import XGBRegressor
   xgb_model = XGBRegressor(tree_method='gpu_hist', random_state=42)
   xgb_model.fit(X_train, y_train)
   ```

---

### üìâ **Nota Final**  
La transformaci√≥n logar√≠tmica es clave para manejar la cola larga de precios. Este c√≥digo optimiza el equilibrio entre **precisi√≥n** y **generalizaci√≥n**.
"""

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.preprocessing import RobustScaler

# --- 0. Copia del DataFrame y limpieza ---
df_optimized = df_analysis.copy()
columns_to_drop = ['luxury_score', 'price_per_person', 'neighbourhood_price_avg', 'log_price']
df_optimized = df_optimized.drop(columns=columns_to_drop)

# --- 1. Transformaci√≥n logar√≠tmica de 'price' (confirmada por histograma) ---
y = np.log1p(df_optimized['price'])  # Transformaci√≥n obligatoria por cola larga

# --- 2. Ingenier√≠a de features ---
# Interacciones clave
df_optimized['bed_bath_ratio'] = df_optimized['bathrooms'] / (df_optimized['bedrooms'] + 1e-6)
df_optimized['acc_bed_interaction'] = df_optimized['accommodates'] * df_optimized['bedrooms']

# Agrupar categor√≠as raras en 'property_type'
property_type_cols = [col for col in df_optimized.columns if col.startswith('property_type_')]
if len(property_type_cols) > 0:
    rare_properties = df_optimized[property_type_cols].sum()[df_optimized[property_type_cols].sum() < 20].index
    df_optimized['property_type_Other'] = df_optimized[rare_properties].sum(axis=1)
    df_optimized = df_optimized.drop(columns=rare_properties)

# --- 3. Escalado de features num√©ricas ---
numeric_features = ['accommodates', 'bathrooms', 'bedrooms', 'beds', 'minimum_nights', 'number_of_reviews']
scaler = RobustScaler()
df_optimized[numeric_features] = scaler.fit_transform(df_optimized[numeric_features])

# --- 4. Definir X e y ---
X = df_optimized.drop(columns=['price'])
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# --- 5. B√∫squeda de hiperpar√°metros optimizados ---
param_grid_v4 = {
    'max_depth': [15, 20, None],      # Profundidad flexible
    'min_samples_leaf': [2, 3],       # Control de overfitting
    'n_estimators': [200, 300],       # M√°s √°rboles para estabilidad
    'max_features': ['sqrt', 0.5],    # Balance entre features
    'max_samples': [0.8, None]        # Submuestreo para diversidad
}

grid_search_v4 = GridSearchCV(
    RandomForestRegressor(random_state=42),
    param_grid_v4,
    cv=5,
    scoring='r2',
    n_jobs=-1
)
grid_search_v4.fit(X_train, y_train)

# --- 6. Evaluaci√≥n del mejor modelo ---
best_model_v4 = grid_search_v4.best_estimator_
y_pred_train = best_model_v4.predict(X_train)
y_pred_test = best_model_v4.predict(X_test)

# Revertir transformaci√≥n logar√≠tmica para m√©tricas
y_train_exp = np.expm1(y_train)
y_test_exp = np.expm1(y_test)
y_pred_train_exp = np.expm1(y_pred_train)
y_pred_test_exp = np.expm1(y_pred_test)

# --- 7. M√©tricas en escala original (EUR) ---
def print_metrics(y_true, y_pred, dataset_name):
    r2 = r2_score(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    print(f"\nüìä **M√©tricas para {dataset_name} (EUR)**")
    print(f"R¬≤:    {r2:.4f} | MSE:   {mse:.2f}")
    print(f"RMSE:  {rmse:.2f} | MAE:   {mae:.2f}")

print("‚úÖ **Mejores par√°metros (v4):**", grid_search_v4.best_params_)
print_metrics(y_train_exp, y_pred_train_exp, "Entrenamiento")
print_metrics(y_test_exp, y_pred_test_exp, "Prueba")

# --- 8. Feature Importances ---
importances = best_model_v4.feature_importances_
top_features = pd.DataFrame({'Feature': X.columns, 'Importance': importances}) \
    .sort_values(by='Importance', ascending=False).head(10)
print("\nüîù **Top 10 Features m√°s importantes:**")
print(top_features.to_markdown(tablefmt="grid", index=False))

"""### üìå **C√≥digo Final Optimizado (Random Forest v5)**
**Mejoras clave:**  
- Feature engineering avanzado (a√±os de experiencia del host, reviews por mes).  
- Optimizaci√≥n de tiempo de ejecuci√≥n (timeout ajustado).  
- Hiperpar√°metros afinados.
"""

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.preprocessing import RobustScaler

# --- 0. Copia del DataFrame y limpieza ---
# Cargar el DataFrame de an√°lisis
from pathlib import Path
processed_data_dir = Path("../data/processed/")
df_analysis = pd.read_csv(processed_data_dir / "df_analysis.csv")
df_optimized = df_analysis.copy()
columns_to_drop = ['luxury_score', 'price_per_person', 'neighbourhood_price_avg', 'log_price']
df_optimized = df_optimized.drop(columns=columns_to_drop)
# Volcar df_optimized a CSV
# Aseg√∫rate de que el directorio existe
processed_data_dir = Path("../data/processed/")
processed_data_dir.mkdir(parents=True, exist_ok=True)
df_optimized.to_csv(processed_data_dir / "df_optimized.csv", index=False)
print(f"‚úì Saved to {processed_data_dir / 'df_optimized.csv'}")
print("="*80)

# --- 1. Transformaci√≥n logar√≠tmica de 'price' ---
y = np.log1p(df_optimized['price'])

# --- 2. Ingenier√≠a de features avanzada ---
# Interacciones
df_optimized['bed_bath_ratio'] = df_optimized['bathrooms'] / (df_optimized['bedrooms'] + 1e-6)
df_optimized['acc_bed_interaction'] = df_optimized['accommodates'] * df_optimized['bedrooms']

# Nuevas features propuestas
df_optimized['host_experience_years'] = 2023 - df_optimized['host_since_year']
df_optimized['reviews_per_month'] = df_optimized['number_of_reviews'] / 12  # Ajustar si hay columna 'months_active'

# Agrupar categor√≠as raras
property_type_cols = [col for col in df_optimized.columns if col.startswith('property_type_')]
if len(property_type_cols) > 0:
    rare_properties = df_optimized[property_type_cols].sum()[df_optimized[property_type_cols].sum() < 20].index
    df_optimized['property_type_Other'] = df_optimized[rare_properties].sum(axis=1)
    df_optimized = df_optimized.drop(columns=rare_properties)

# --- 3. Escalado robusto ---
numeric_features = ['accommodates', 'bathrooms', 'bedrooms', 'beds', 'minimum_nights', 'number_of_reviews', 'host_experience_years']
scaler = RobustScaler()
df_optimized[numeric_features] = scaler.fit_transform(df_optimized[numeric_features])

# --- 4. Definir X e y ---
X = df_optimized.drop(columns=['price', 'host_since_year'])  # Eliminar columna original de a√±o
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# --- 5. B√∫squeda de hiperpar√°metros (optimizada para tiempo) ---
param_grid_v5 = {
    'max_depth': [None, 20],
    'min_samples_leaf': [1, 2],
    'n_estimators': [300],
    'max_features': [0.5],
    'max_samples': [0.8]
}

grid_search_v5 = GridSearchCV(
    RandomForestRegressor(random_state=42),
    param_grid_v5,
    cv=5,
    scoring='r2',
    n_jobs=4  # Reducir workers para evitar timeout
)
grid_search_v5.fit(X_train, y_train)

# --- 6. Evaluaci√≥n ---
best_model_v5 = grid_search_v5.best_estimator_
y_pred_train = best_model_v5.predict(X_train)
y_pred_test = best_model_v5.predict(X_test)

# Revertir transformaci√≥n logar√≠tmica
y_train_exp, y_test_exp = np.expm1(y_train), np.expm1(y_test)
y_pred_train_exp, y_pred_test_exp = np.expm1(y_pred_train), np.expm1(y_pred_test)

# --- 7. M√©tricas ---
def print_metrics(y_true, y_pred, dataset_name):
    r2 = r2_score(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    print(f"\nüìä **M√©tricas para {dataset_name} (USD)**")
    print(f"R¬≤:    {r2:.4f} | MSE:   {mse:.2f}")
    print(f"RMSE:  {rmse:.2f} | MAE:   {mae:.2f}")

print("‚úÖ **Mejores par√°metros (v5):**", grid_search_v5.best_params_)
print_metrics(y_train_exp, y_pred_train_exp, "Entrenamiento")
print_metrics(y_test_exp, y_pred_test_exp, "Prueba")

# --- 8. Feature Importances ---
importances = best_model_v5.feature_importances_
top_features = pd.DataFrame({'Feature': X.columns, 'Importance': importances}) \
    .sort_values(by='Importance', ascending=False).head(10)
print("\nüîù **Top 10 Features m√°s importantes (v5):**")
print(top_features.to_markdown(tablefmt="grid", index=False))

"""---

### üöÄ **C√≥digo para XGBoost (Comparaci√≥n Directa)**
**Ventajas:**  
- Mayor velocidad de entrenamiento.  
- Potencial mejor R¬≤ con tuning adecuado.  

---

### üìä **Comparativa Esperada**
| Modelo          | R¬≤ Test (Esperado) | MAE Test (Esperado) | Tiempo Entrenamiento |
|----------------|--------------------|---------------------|----------------------|
| Random Forest  | 0.77 - 0.79        | 19 - 21             | Alto (~10-15 min)    |
| XGBoost        | **0.78 - 0.82**    | **18 - 20**         | Bajo (~2-5 min)      |

**Recomendaci√≥n:**  
- Usa **Random Forest** si priorizas interpretabilidad (importancia de features).  
- Usa **XGBoost** si buscas m√°xima precisi√≥n y velocidad.  

Ambos c√≥digos incluyen:  
- Transformaci√≥n logar√≠tmica reversible.  
- M√©tricas en USD.  
- Identificaci√≥n de features clave.
"""

import xgboost as xgb
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import numpy as np

# --- 1. Preparar datos ---
# (Usar mismo X_train/X_test que en Random Forest)
# Asegurar que todas las variables categ√≥ricas est√°n codificadas

# --- 2. Entrenamiento con hiperpar√°metros base (CPU) ---
xgb_model = xgb.XGBRegressor(
    objective='reg:squarederror',
    n_estimators=300,
    max_depth=6,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.5,
    random_state=42,
    tree_method='hist'  # Cambiado a CPU (hist/histogram)
)
xgb_model.fit(X_train, y_train)

# --- 3. Predicciones ---
y_pred_train_xgb = xgb_model.predict(X_train)
y_pred_test_xgb = xgb_model.predict(X_test)

# Revertir transformaci√≥n logar√≠tmica
y_pred_train_xgb_exp = np.expm1(y_pred_train_xgb)
y_pred_test_xgb_exp = np.expm1(y_pred_test_xgb)

# --- 4. M√©tricas ---
def print_metrics(y_true, y_pred, dataset_name):
    r2 = r2_score(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    print(f"\nüìä **M√©tricas para {dataset_name} (USD)**")
    print(f"R¬≤:    {r2:.4f} | MSE:   {mse:.2f}")
    print(f"RMSE:  {rmse:.2f} | MAE:   {mae:.2f}")

print("\n‚ö° **M√©tricas para XGBoost (Base)**")
print_metrics(y_train_exp, y_pred_train_xgb_exp, "Entrenamiento")
print_metrics(y_test_exp, y_pred_test_xgb_exp, "Prueba")

# --- 5. Feature Importances ---
xgb_importances = xgb_model.feature_importances_
top_features_xgb = pd.DataFrame({'Feature': X.columns, 'Importance': xgb_importances}) \
    .sort_values(by='Importance', ascending=False).head(10)
print("\nüîù **Top 10 Features (XGBoost Base):**")
print(top_features_xgb.to_markdown(tablefmt="grid", index=False))

# --- 6. Optimizaci√≥n con GridSearch (Opcional) ---
param_grid_xgb = {
    'max_depth': [4, 6, 8],
    'learning_rate': [0.05, 0.1],
    'n_estimators': [200, 300],
    'subsample': [0.8, 1.0]
}

grid_search_xgb = GridSearchCV(
    xgb.XGBRegressor(objective='reg:squarederror', random_state=42, tree_method='hist'),
    param_grid_xgb,
    cv=5,
    scoring='r2',
    n_jobs=4
)
grid_search_xgb.fit(X_train, y_train)

# --- 7. M√©tricas del modelo optimizado ---
print("\n‚úÖ **Mejores par√°metros (XGBoost Optimizado):**", grid_search_xgb.best_params_)
y_pred_train_xgb_opt = grid_search_xgb.predict(X_train)
y_pred_test_xgb_opt = grid_search_xgb.predict(X_test)
y_pred_train_xgb_opt_exp = np.expm1(y_pred_train_xgb_opt)
y_pred_test_xgb_opt_exp = np.expm1(y_pred_test_xgb_opt)

print_metrics(y_train_exp, y_pred_train_xgb_opt_exp, "Entrenamiento (Optimizado)")
print_metrics(y_test_exp, y_pred_test_xgb_opt_exp, "Prueba (Optimizado)")

# --- 8. Feature Importances (Optimizado) ---
xgb_importances_opt = grid_search_xgb.best_estimator_.feature_importances_
top_features_xgb_opt = pd.DataFrame({'Feature': X.columns, 'Importance': xgb_importances_opt}) \
    .sort_values(by='Importance', ascending=False).head(10)
print("\nüîù **Top 10 Features (XGBoost Optimizado):**")
print(top_features_xgb_opt.to_markdown(tablefmt="grid", index=False))

"""### üîç **An√°lisis de Resultados: XGBoost vs Random Forest**

#### üìä **Comparativa de M√©tricas**
| Modelo               | R¬≤ Train | R¬≤ Test  | MAE Test | Tiempo Estimado |
|----------------------|----------|----------|----------|-----------------|
| **Random Forest v5** | 0.89     | **0.76** | 20.35    | ~15 min         |
| **XGBoost (Base)**   | 0.71     | 0.67     | 25.13    | ~3 min          |
| **XGBoost (Optimizado)** | 0.81 | **0.72** | 22.77    | ~10 min         |

#### üéØ **Conclusiones Clave**
1. **Random Forest sigue siendo mejor**:
   - Mayor R¬≤ en test (0.76 vs 0.72 de XGBoost optimizado).
   - Menor MAE (20.35 vs 22.77).
   - M√°s equilibrado (diferencia train-test: 13% vs 9% en XGBoost optimizado).

2. **Problema con XGBoost**:
   - **Overfitting en la versi√≥n base** (R¬≤ train 0.71 vs test 0.67).
   - **Importancia de features desbalanceada** (`room_type_Entire_home_apt` domina con 86% en optimizado).

3. **¬øPor qu√© dos resultados en XGBoost?**:
   - **Primer bloque**: Modelo base con par√°metros por defecto.
   - **Segundo bloque**: Modelo optimizado con GridSearch (mejores hiperpar√°metros).

---

### üöÄ **Recomendaci√≥n Final**
**Qu√©date con Random Forest** porque:  
‚úÖ **Mayor precisi√≥n** en test (R¬≤ 0.76 vs 0.72).  
‚úÖ **Errores m√°s bajos** (MAE 20.35 vs 22.77).  
‚úÖ **Interpretabilidad** (importancia de features m√°s balanceada).  

**Usa XGBoost solo si:**  
‚û° Necesitas velocidad (es m√°s r√°pido con datasets grandes).  
‚û° Quieres experimentar con otros par√°metros (ej: reducir `max_depth` a 4-6).  

---

### üìå **Pasos para Mejorar XGBoost (Opcional)**
Si decides seguir con XGBoost, ajusta estos par√°metros:
```python
xgb_model_v2 = xgb.XGBRegressor(
    objective='reg:squarederror',
    n_estimators=200,
    max_depth=4,  # Reducir profundidad
    learning_rate=0.05,  # Tasa de aprendizaje m√°s baja
    subsample=0.7,
    colsample_bytree=0.3,  # Menos features por √°rbol
    random_state=42,
    tree_method='hist'
)
```
**Objetivo:** Reducir overfitting y balancear importancia de features.

---

### üî• **C√≥digo Definitivo (Random Forest v5)**
```python
# Usa el c√≥digo de Random Forest v5 anterior (el que te dio R¬≤ test = 0.76)
# Es tu mejor modelo actualmente.
```

**Nota:** Si el tiempo de entrenamiento de Random Forest es un problema, considera:
- Reducir `n_estimators` a 200.
- Usar `max_samples=0.7` para acelerar.  

¬°El modelo est√° listo para producci√≥n! üéâ

### üî• **C√≥digo Optimizado (XGBoost v2)**
"""

import xgboost as xgb
from sklearn.metrics import r2_score, root_mean_squared_error, mean_absolute_error
import numpy as np

# --- 1. Preparar datos ---
# (Usar mismo X_train/X_test que en Random Forest)
# Asegurar que todas las variables categ√≥ricas est√°n codificadas

# --- 2. Entrenamiento con hiperpar√°metros base (CPU) ---
xgb_model_v2 = xgb.XGBRegressor(
    objective='reg:squarederror',
    n_estimators=200,
    max_depth=4,  # Reducir profundidad
    learning_rate=0.05,  # Tasa de aprendizaje m√°s baja
    subsample=0.7,
    colsample_bytree=0.3,  # Menos features por √°rbol
    random_state=42,
    tree_method='hist'
)
xgb_model_v2.fit(X_train, y_train)

# --- 3. Predicciones ---
y_pred_train_xgb = xgb_model_v2.predict(X_train)
y_pred_test_xgb = xgb_model_v2.predict(X_test)

# Revertir transformaci√≥n logar√≠tmica
y_pred_train_xgb_exp = np.expm1(y_pred_train_xgb)
y_pred_test_xgb_exp = np.expm1(y_pred_test_xgb)

# --- 4. M√©tricas ---
def print_metrics(y_true, y_pred, dataset_name):
    r2 = r2_score(y_true, y_pred)
    mse = root_mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    print(f"\nüìä **M√©tricas para {dataset_name} (EUR)**")
    print(f"R¬≤:    {r2:.4f} | MSE:   {mse:.2f}")
    print(f"RMSE:  {rmse:.2f} | MAE:   {mae:.2f}")

print("\n‚ö° **M√©tricas para XGBoost (Base)**")
print_metrics(y_train_exp, y_pred_train_xgb_exp, "Entrenamiento")
print_metrics(y_test_exp, y_pred_test_xgb_exp, "Prueba")

# --- 5. Feature Importances ---
xgb_importances = xgb_model_v2.feature_importances_
top_features_xgb = pd.DataFrame({'Feature': X.columns, 'Importance': xgb_importances}) \
    .sort_values(by='Importance', ascending=False).head(10)
print("\nüîù **Top 10 Features (XGBoost Base):**")
print(top_features_xgb.to_markdown(tablefmt="grid", index=False))

# --- 6. Optimizaci√≥n con GridSearch (Opcional) ---
param_grid_xgb = {
    'max_depth': [4, 6, 8],
    'learning_rate': [0.05, 0.1],
    'n_estimators': [200, 300],
    'subsample': [0.8, 1.0]
}

grid_search_xgb = GridSearchCV(
    xgb.XGBRegressor(objective='reg:squarederror', random_state=42, tree_method='hist'),
    param_grid_xgb,
    cv=5,
    scoring='r2',
    n_jobs=4
)
grid_search_xgb.fit(X_train, y_train)

# --- 7. M√©tricas del modelo optimizado ---
print("\n‚úÖ **Mejores par√°metros (XGBoost Optimizado):**", grid_search_xgb.best_params_)
y_pred_train_xgb_opt = grid_search_xgb.predict(X_train)
y_pred_test_xgb_opt = grid_search_xgb.predict(X_test)
y_pred_train_xgb_opt_exp = np.expm1(y_pred_train_xgb_opt)
y_pred_test_xgb_opt_exp = np.expm1(y_pred_test_xgb_opt)

print_metrics(y_train_exp, y_pred_train_xgb_opt_exp, "Entrenamiento (Optimizado)")
print_metrics(y_test_exp, y_pred_test_xgb_opt_exp, "Prueba (Optimizado)")

# --- 8. Feature Importances (Optimizado) ---
xgb_importances_opt = grid_search_xgb.best_estimator_.feature_importances_
top_features_xgb_opt = pd.DataFrame({'Feature': X.columns, 'Importance': xgb_importances_opt}) \
    .sort_values(by='Importance', ascending=False).head(10)
print("\nüîù **Top 10 Features (XGBoost Optimizado):**")
print(top_features_xgb_opt.to_markdown(tablefmt="grid", index=False))

"""### üöÄ **Random Forest v6 - C√≥digo Optimizado para Mejorar R¬≤ Test (0.77 ‚Üí 0.80+)**

Versi√≥n mejorada con **feature engineering estrat√©gico** y **ajuste de hiperpar√°metros** para maximizar el R¬≤ en test, manteniendo la estructura de tus c√≥digos anteriores:

---

### üìå **Mejoras Clave Respecto a v5**
1. **Nuevas Features Interactivas**:
   - `bed_bath_ratio`: Captura la relaci√≥n entre ba√±os y habitaciones.
   - `acc_to_beds`: Ratio de capacidad vs camas disponibles.
   - `reviews_per_month`: Din√°mica de actividad del listado.

2. **Ajuste de Hiperpar√°metros**:
   - `max_features=0.33` (en lugar de 0.5) ‚Üí Mayor generalizaci√≥n.
   - `min_samples_leaf=3` ‚Üí Reduce overfitting.
   - `n_estimators=300` ‚Üí M√°s estabilidad.

3. **Escalado Robusto**:
   - Aplicado solo a features num√©ricas cr√≠ticas (evita distorsi√≥n en categ√≥ricas).

4. **M√©tricas en EUR**:
   - Todas las m√©tricas se reportan en escala original (tras revertir `log1p`).

---

### üìà **Resultados Esperados**
| M√©trica       | v5 (Anterior) | v6 (Esperado) |
|--------------|---------------|---------------|
| **R¬≤ Test**  | 0.76          | **0.78-0.80** |
| **MAE Test** | 20.35         | **18-19**     |

---

### üîç **¬øPor Qu√© Funciona Mejor?**
- **Interacciones no lineales**: Las nuevas features capturan relaciones complejas entre variables.
- **Control de overfitting**: Par√°metros m√°s restrictivos (`max_features=0.33`, `min_samples_leaf=3`).
- **Escalado inteligente**: RobustScaler protege contra outliers sin afectar relaciones no lineales.

---

### üö® **Si el R¬≤ No Mejora**
1. **Verifica fugas de datos**:
   ```python
   print([col for col in X.columns if 'price' in col.lower()])
   ```
2. **Prueba reducir m√°s `max_features`** (ej: 0.25).
3. **A√±ade m√°s datos** si es posible (el tama√±o de muestra afecta directamente al R¬≤).
"""

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.preprocessing import RobustScaler

# --- 0. Cargar datos y selecci√≥n de features ---
df_reduced = df_analysis.copy()

# Lista de features relevantes (sin fugas)
features_relevantes = [
    'accommodates', 'bathrooms', 'bedrooms', 'beds', 'minimum_nights',
    'number_of_reviews', 'review_scores_rating', 'host_is_superhost',
    'host_since_year', 'neighbourhood_density', 'host_experience',
    'has_wifi', 'has_air_conditioning', 'has_pool', 'has_kitchen', 'has_washer'
]

# --- 1. Ingenier√≠a de features AVANZADA ---
# Interacciones clave
df_reduced['bed_bath_ratio'] = df_reduced['bathrooms'] / (df_reduced['bedrooms'] + 1e-6)
df_reduced['acc_to_beds'] = df_reduced['accommodates'] / (df_reduced['beds'] + 1e-6)
df_reduced['reviews_per_month'] = df_reduced['number_of_reviews'] / 12  # Asumiendo 1 a√±o de antig√ºedad m√≠nima

# Experiencia del host (mejorada)
# Calculate host experience years and drop 'host_since_year' only after all operations are completed
df_reduced['host_experience_years'] = 2023 - df_reduced['host_since_year']

# Ensure 'host_since_year' is dropped only after all operations requiring it are done
if 'host_since_year' in df_reduced.columns:
    df_reduced.drop(columns=['host_since_year'], inplace=True)

# --- 2. Transformaci√≥n logar√≠tmica del target ---
y = np.log1p(df_reduced['price'])  # Para manejar cola larga
# Cargar el archivo CSV para obtener la columna necesaria
processed_data_dir = Path("../data/processed/")
df_analysis_path = processed_data_dir / "df_analysis.csv"
df_analysis_full = pd.read_csv(df_analysis_path)

# Asegurarse de que la columna 'host_since_year' est√© presente
if 'host_since_year' in df_analysis_full.columns:
    df_reduced['host_since_year'] = df_analysis_full['host_since_year']
else:
    raise KeyError("La columna 'host_since_year' no est√° presente en el archivo df_analysis.csv")

# Seleccionar las columnas relevantes
X = df_reduced[features_relevantes + ['bed_bath_ratio', 'acc_to_beds', 'reviews_per_month', 'host_experience_years']]

# --- 3. Escalado robusto de num√©ricas ---
numeric_features = ['accommodates', 'bathrooms', 'bedrooms', 'beds', 'minimum_nights',
                   'number_of_reviews', 'neighbourhood_density', 'host_experience_years']
scaler = RobustScaler()
X[numeric_features] = scaler.fit_transform(X[numeric_features])

# --- 4. Divisi√≥n train-test ---
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# --- 5. Modelo RandomForest OPTIMIZADO (v6) ---
model_v6 = RandomForestRegressor(
    n_estimators=300,          # M√°s √°rboles para estabilidad
    max_depth=None,            # Profundidad ilimitada (controlada por min_samples_leaf)
    min_samples_leaf=3,        # Reducir overfitting
    max_features=0.33,         # Menos features por √°rbol (mejor generalizaci√≥n)
    max_samples=0.8,           # Submuestreo para diversidad
    random_state=42,
    n_jobs=-1
)
model_v6.fit(X_train, y_train)

# --- 6. Predicciones y m√©tricas ---
def print_metrics(y_true, y_pred, dataset_name):
    y_true_exp = np.expm1(y_true)
    y_pred_exp = np.expm1(y_pred)
    r2 = r2_score(y_true_exp, y_pred_exp)
    mae = mean_absolute_error(y_true_exp, y_pred_exp)
    print(f"\nüìä **{dataset_name}**")
    print(f"R¬≤: {r2:.4f} | MAE: {mae:.2f} USD")

y_pred_train = model_v6.predict(X_train)
y_pred_test = model_v6.predict(X_test)

print_metrics(y_train, y_pred_train, "ENTRENAMIENTO (v6)")
print_metrics(y_test, y_pred_test, "PRUEBA (v6)")

# --- 7. An√°lisis de features ---
importances = model_v6.feature_importances_
top_features = pd.DataFrame({'Feature': X.columns, 'Importance': importances}) \
    .sort_values(by='Importance', ascending=False).head(10)
print("\nüîù **Top 10 Features (v6):**")
print(top_features.to_markdown(tablefmt="grid"))

"""### üîç **An√°lisis de Resultados (v6)**
Los resultados muestran:
- **R¬≤ Train: 0.80** (bueno, pero podr√≠a ser mejor)
- **R¬≤ Test: 0.69** (ligera mejora vs XGBoost, pero por debajo de tu v5)
- **Overfitting**: Diferencia del 11% entre train/test (aceptable pero mejorable)

---

### üöÄ **Estrategias para Mejorar el Modelo (v7)**

#### 1. **Reducir Overfitting**
- **Aumentar `min_samples_leaf`**: De 3 a 5 para mayor generalizaci√≥n.
- **Limitar `max_depth`**: Probar 15-20 en lugar de `None`.
- **Reducir `n_estimators`**: 200 en lugar de 300 (menos √°rboles = menos varianza).

#### 2. **Mejorar Feature Engineering**
- **Agregar interacci√≥n clave**: `review_scores_rating * number_of_reviews` (calidad √ó popularidad).
- **Transformar `minimum_nights`**: Aplicar `np.log1p` si tiene cola larga.
- **Codificar `host_is_superhost` como num√©rico (0/1)** si no lo est√°.

#### 3. **Ajustar Hiperpar√°metros**
```python
param_grid_v7 = {
    'max_depth': [15, 20],          # Limitar profundidad
    'min_samples_leaf': [3, 5],     # M√°s riguroso
    'n_estimators': [200, 250],     # Equilibrio velocidad/performance
    'max_features': [0.3, 0.4],     # M√°s restricci√≥n
    'max_samples': [0.7, 0.8]       # Mayor submuestreo
}
```

---

### üìà **Resultados Esperados (v7)**
| M√©trica       | v6 (Actual) | v7 (Objetivo) |
|--------------|-------------|---------------|
| **R¬≤ Test**  | 0.69        | **0.73-0.75** |
| **MAE Test** | 24.39       | **<22**       |

---

### üîç **Diagn√≥stico Adicional**
Si el R¬≤ no mejora:
1. **Verifica correlaciones**:
   ```python
   print(X.corrwith(np.expm1(y)).sort_values(ascending=False))
   ```
2. **Prueba eliminar features poco importantes** (importancia < 0.01).
3. **Considera t√©cnicas avanzadas**:
   - **Stacking**: Combina RandomForest con XGBoost.
   - **Embeddings categ√≥ricos**: Para `neighbourhood_density`.

---

### üéØ **Conclusi√≥n**
El c√≥digo v7 est√° optimizado para **maximizar el R¬≤ en test** sin overfitting. Si tras ejecutarlo no alcanzas el 0.75, ser√≠a recomendable:
1. Revisar leakage de datos (¬øalguna variable contiene info de `price`?).  
2. Aumentar el tama√±o del dataset (si es posible).  
3. Probar modelos alternativos (Gradient Boosting o redes neuronales).

### üìå **C√≥digo Optimizado (Random Forest v7)**
"""

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import r2_score, mean_absolute_error
from sklearn.preprocessing import RobustScaler

# --- 0. Preparaci√≥n de datos ---
df_reduced = df_analysis.copy()

# Feature engineering mejorado
df_reduced['bed_bath_ratio'] = df_reduced['bathrooms'] / (df_reduced['bedrooms'] + 1e-6)
df_reduced['acc_to_beds'] = df_reduced['accommodates'] / (df_reduced['beds'] + 1e-6)
df_reduced['reviews_per_month'] = df_reduced['number_of_reviews'] / 12
df_reduced['rating_popularity'] = df_reduced['review_scores_rating'] * np.log1p(df_reduced['number_of_reviews'])
df_reduced['host_experience_years'] = 2023 - df_reduced['host_since_year']
df_reduced['minimum_nights_log'] = np.log1p(df_reduced['minimum_nights'])

# Selecci√≥n final de features
features_v7 = [
    'accommodates', 'bathrooms', 'bedrooms', 'beds', 'minimum_nights_log',
    'number_of_reviews', 'review_scores_rating', 'host_is_superhost',
    'neighbourhood_density', 'host_experience_years', 'has_wifi',
    'has_air_conditioning', 'bed_bath_ratio', 'acc_to_beds',
    'reviews_per_month', 'rating_popularity'
]

X = df_reduced[features_v7]
y = np.log1p(df_reduced['price'])

# Escalado robusto
numeric_features = ['accommodates', 'bathrooms', 'bedrooms', 'beds', 'minimum_nights_log',
                   'number_of_reviews', 'neighbourhood_density', 'host_experience_years']
scaler = RobustScaler()
X[numeric_features] = scaler.fit_transform(X[numeric_features])

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# --- Modelo y GridSearch ---
param_grid_v7 = {
    'max_depth': [15, 20],
    'min_samples_leaf': [3, 5],
    'n_estimators': [200, 250],
    'max_features': [0.3, 0.4],
    'max_samples': [0.7, 0.8]
}

grid_search_v7 = GridSearchCV(
    RandomForestRegressor(random_state=42, n_jobs=-1),
    param_grid_v7,
    cv=5,
    scoring='r2'
)
grid_search_v7.fit(X_train, y_train)

# --- Evaluaci√≥n ---
best_model_v7 = grid_search_v7.best_estimator_
y_pred_test = best_model_v7.predict(X_test)

def print_metrics(y_true, y_pred):
    y_true_exp = np.expm1(y_true)
    y_pred_exp = np.expm1(y_pred)
    r2 = r2_score(y_true_exp, y_pred_exp)
    mae = mean_absolute_error(y_true_exp, y_pred_exp)
    print(f"R¬≤ Test: {r2:.4f} | MAE Test: {mae:.2f} EUR")

print("‚úÖ Mejores par√°metros:", grid_search_v7.best_params_)
print_metrics(y_test, y_pred_test)

"""----------------"""