# -*- coding: utf-8 -*-
"""
Archivo original:
inside_airbnb_eda_3.ipynb

Automatically generated by Colab.
Original file is located at
    https://colab.research.google.com/drive/1KRhRe3dzSHb5lh_C_reVKLgjMI0jvLum

Cargar CSV, entrenar modelo Random Forest y guardar en pickle
Este script entrena un modelo de Random Forest para predecir el precio de los alojamientos
en Airbnb. Se utiliza un conjunto de datos optimizado y se aplican transformaciones logar√≠tmicas al target.
El modelo se eval√∫a utilizando m√©tricas como R¬≤ y MAE, y se guarda en un archivo pickle junto con el scaler utilizado.
El script incluye los siguientes pasos:
1. Cargar datos procesados desde un CSV.
2. Aplicar transformaci√≥n logar√≠tmica al target (precio).
3. Definir las caracter√≠sticas (features) para el modelo.
4. Escalar las caracter√≠sticas num√©ricas utilizando RobustScaler.
5. Dividir los datos en conjuntos de entrenamiento y prueba.
6. Entrenar un modelo de Random Forest con los datos de entrenamiento.
7. Evaluar el modelo utilizando m√©tricas como R¬≤ y MAE.
8. Guardar el modelo y el scaler en archivos pickle para su uso posterior.

El script utiliza bibliotecas como pandas, numpy, scikit-learn y pickle para realizar estas tareas.
El modelo se entrena con 300 √°rboles y se ajusta para evitar el sobreajuste.
El scaler se utiliza para normalizar las caracter√≠sticas num√©ricas y mejorar la convergencia del modelo.
El modelo se guarda en un directorio espec√≠fico para su uso posterior.
El script est√° dise√±ado para ser ejecutado en un entorno de Python y requiere la instalaci√≥n de las bibliotecas necesarias.
"""

# -*- coding: utf-8 -*-
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.preprocessing import RobustScaler
import pickle
from pathlib import Path

# --- Configuraci√≥n de paths ---
processed_data_dir = Path("data/processed/")
model_dir = Path("models/")
model_dir.mkdir(parents=True, exist_ok=True)  # Crear directorio si no existe

# --- 1. Cargar datos procesados ---
df_optimized = pd.read_csv(processed_data_dir / "df_optimized.csv")

# --- 2. Transformaci√≥n logar√≠tmica del target (price) ---
y = np.log1p(df_optimized['price'])

# --- 3. Definir features (X) ---
# Eliminar columnas no relevantes para el modelo
X = df_optimized.drop(columns=['price', 'host_since_year'])  # Ajusta seg√∫n tu CSV

# --- 4. Escalado num√©rico (opcional, pero recomendado) ---
numeric_features = ['accommodates', 'bathrooms', 'bedrooms', 'beds', 'minimum_nights', 'number_of_reviews']
scaler = RobustScaler()
X[numeric_features] = scaler.fit_transform(X[numeric_features])

# --- 5. Divisi√≥n train-test (para evaluaci√≥n) ---
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# --- 6. Entrenar modelo Random Forest v5 ---
random_forest_v5 = RandomForestRegressor(
    n_estimators=300,
    max_depth=None,
    min_samples_leaf=2,
    max_features=0.5,
    max_samples=0.8,
    random_state=42,
    n_jobs=-1
)
random_forest_v5.fit(X_train, y_train)

# --- 7. Evaluaci√≥n del modelo ---
def print_metrics(y_true, y_pred, dataset_name):
    y_true_exp = np.expm1(y_true)
    y_pred_exp = np.expm1(y_pred)
    r2 = r2_score(y_true_exp, y_pred_exp)
    mae = mean_absolute_error(y_true_exp, y_pred_exp)
    print(f"\nüìä **M√©tricas para {dataset_name}**")
    print(f"R¬≤: {r2:.4f} | MAE: {mae:.2f} EUR")

y_pred_train = random_forest_v5.predict(X_train)
y_pred_test = random_forest_v5.predict(X_test)
print_metrics(y_train, y_pred_train, "ENTRENAMIENTO")
print_metrics(y_test, y_pred_test, "PRUEBA")

# --- 8. Guardar modelo y scaler en pickle ---
with open(model_dir / "random_forest_v5.pkl", 'wb') as f:
    pickle.dump(random_forest_v5, f)

with open(model_dir / "scaler.pkl", 'wb') as f:
    pickle.dump(scaler, f)

print(f"\n‚úÖ Modelo y scaler guardados en: {model_dir}/")