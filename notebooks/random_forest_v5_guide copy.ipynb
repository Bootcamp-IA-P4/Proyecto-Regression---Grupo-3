{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4acb64eb",
   "metadata": {},
   "source": [
    "# **Guía de como Entrenar el Modelo, Guardarlo en un archivo pickle y como usarlo**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c784f42",
   "metadata": {},
   "source": [
    "Para transformar el archivo `inside_airbnb_model.py` (que originalmente era un notebook) en un script Python que cargue el archivo `df_optimized.csv`, entrene el modelo v5 y lo guarde en formato pickle, sigue estos pasos:\n",
    "\n",
    "### **Script Python (`train_model_v5.py`)**\n",
    "```python\n",
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Configuración de paths ---\n",
    "processed_data_dir = Path(\"../data/processed/\")\n",
    "model_dir = Path(\"../models/\")\n",
    "model_dir.mkdir(parents=True, exist_ok=True)  # Crear directorio si no existe\n",
    "\n",
    "# --- 1. Cargar datos procesados ---\n",
    "df_optimized = pd.read_csv(processed_data_dir / \"df_optimized.csv\")\n",
    "\n",
    "# --- 2. Transformación logarítmica del target (price) ---\n",
    "y = np.log1p(df_optimized['price'])\n",
    "\n",
    "# --- 3. Definir features (X) ---\n",
    "# Eliminar columnas no relevantes para el modelo\n",
    "X = df_optimized.drop(columns=['price', 'host_since_year'])  # Ajusta según tu CSV\n",
    "\n",
    "# --- 4. Escalado numérico (opcional, pero recomendado) ---\n",
    "numeric_features = ['accommodates', 'bathrooms', 'bedrooms', 'beds', 'minimum_nights', 'number_of_reviews']\n",
    "scaler = RobustScaler()\n",
    "X[numeric_features] = scaler.fit_transform(X[numeric_features])\n",
    "\n",
    "# --- 5. División train-test (para evaluación) ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- 6. Entrenar modelo Random Forest v5 ---\n",
    "model_v5 = RandomForestRegressor(\n",
    "    n_estimators=300,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=2,\n",
    "    max_features=0.5,\n",
    "    max_samples=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "model_v5.fit(X_train, y_train)\n",
    "\n",
    "# --- 7. Evaluación del modelo ---\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    y_true_exp = np.expm1(y_true)\n",
    "    y_pred_exp = np.expm1(y_pred)\n",
    "    r2 = r2_score(y_true_exp, y_pred_exp)\n",
    "    mae = mean_absolute_error(y_true_exp, y_pred_exp)\n",
    "    print(f\"\\n📊 **Métricas para {dataset_name}**\")\n",
    "    print(f\"R²: {r2:.4f} | MAE: {mae:.2f} EUR\")\n",
    "\n",
    "y_pred_train = model_v5.predict(X_train)\n",
    "y_pred_test = model_v5.predict(X_test)\n",
    "print_metrics(y_train, y_pred_train, \"ENTRENAMIENTO\")\n",
    "print_metrics(y_test, y_pred_test, \"PRUEBA\")\n",
    "\n",
    "# --- 8. Guardar modelo y scaler en pickle ---\n",
    "with open(model_dir / \"model_v5.pkl\", 'wb') as f:\n",
    "    pickle.dump(model_v5, f)\n",
    "\n",
    "with open(model_dir / \"scaler.pkl\", 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(f\"\\n✅ Modelo y scaler guardados en: {model_dir}/\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Explicación Paso a Paso**\n",
    "1. **Carga de datos**:  \n",
    "   - Lee el archivo `df_optimized.csv` desde la ruta `../data/processed/`.\n",
    "\n",
    "2. **Preprocesamiento**:  \n",
    "   - Aplica transformación logarítmica a `price` para manejar colas largas.  \n",
    "   - Escala features numéricas con `RobustScaler` (protege contra outliers).\n",
    "\n",
    "3. **Entrenamiento del modelo**:  \n",
    "   - Usa los hiperparámetros óptimos del **modelo v5** (`n_estimators=300`, `max_features=0.5`, etc.).  \n",
    "   - Evalúa con métricas en escala original (EUR).\n",
    "\n",
    "4. **Exportación a pickle**:  \n",
    "   - Guarda el modelo entrenado (`model_v5.pkl`) y el scaler (`scaler.pkl`) en la carpeta `../models/`.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "### **Notas Clave**\n",
    "- **Ajusta las columnas**: Verifica que `X` no incluya columnas irrelevantes o con leakage (como `log_price`).  \n",
    "- **Entorno reproducible**: Usa `random_state=42` para resultados consistentes.  \n",
    "- **Requisitos**: Asegúrate de tener las mismas versiones de librerías (`scikit-learn`, `pandas`, etc.) en entrenamiento y producción.  \n",
    "\n",
    "Si necesitas adaptar algo específico (como las features usadas), modifica el script según tu caso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f67ca72",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbc038a",
   "metadata": {},
   "source": [
    "## **Como usar el Modelo**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f41a56",
   "metadata": {},
   "source": [
    "Basado en el script de entrenamiento, aquí tienes cómo probar correctamente el modelo guardado (`random_forest_v5.pkl`):\n",
    "\n",
    "### **Script completo para probar el modelo (`test_model.py`)**\n",
    "```python\n",
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# --- Configuración de paths ---\n",
    "model_dir = Path(\"models/\")\n",
    "processed_data_dir = Path(\"data/processed/\")\n",
    "\n",
    "# --- 1. Cargar modelo y scaler ---\n",
    "with open(model_dir / \"random_forest_v5.pkl\", 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "    \n",
    "with open(model_dir / \"scaler.pkl\", 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "\n",
    "# --- 2. Cargar datos de prueba (o nuevos datos) ---\n",
    "# Opción A: Usar datos de prueba guardados (si existen)\n",
    "# df_test = pd.read_csv(processed_data_dir / \"test_data.csv\")\n",
    "\n",
    "# Opción B: Crear datos de ejemplo (simulando entrada)\n",
    "test_data = {\n",
    "    'accommodates': [2, 4],  # Ejemplo: 2 personas\n",
    "    'bathrooms': [1.0, 2.0],\n",
    "    'bedrooms': [1, 2],\n",
    "    'beds': [1, 3],\n",
    "    'minimum_nights': [3, 2],\n",
    "    'number_of_reviews': [10, 25],\n",
    "    'review_scores_rating': [95, 80],\n",
    "    'host_is_superhost': [1, 0],  # 1=True, 0=False\n",
    "    'neighbourhood_density': [0.5, 0.3],\n",
    "    'has_wifi': [1, 1],\n",
    "    'has_air_conditioning': [1, 0],\n",
    "    # Añade todas las features usadas en entrenamiento\n",
    "}\n",
    "\n",
    "X_new = pd.DataFrame(test_data)\n",
    "\n",
    "# --- 3. Preprocesamiento igual que en entrenamiento ---\n",
    "# Escalar features numéricas\n",
    "numeric_features = ['accommodates', 'bathrooms', 'bedrooms', 'beds', \n",
    "                   'minimum_nights', 'number_of_reviews']\n",
    "X_new[numeric_features] = scaler.transform(X_new[numeric_features])\n",
    "\n",
    "# --- 4. Predecir ---\n",
    "pred_log = model.predict(X_new)  # Predicciones en escala logarítmica\n",
    "pred_price = np.expm1(pred_log)  # Convertir a precio real (EUR)\n",
    "\n",
    "# --- 5. Resultados ---\n",
    "print(\"\\n🔮 Predicciones:\")\n",
    "for i, price in enumerate(pred_price):\n",
    "    print(f\"Alojamiento {i+1}: ${price:.2f} EUR\")\n",
    "\n",
    "# --- 6. Opcional: Importancia de features ---\n",
    "if hasattr(model, 'feature_importances_'):\n",
    "    print(\"\\n📊 Importancia de features:\")\n",
    "    features = X_new.columns\n",
    "    importances = model.feature_importances_\n",
    "    for feat, imp in sorted(zip(features, importances), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"{feat}: {imp:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **¿Qué hace este script?**\n",
    "1. **Carga el modelo y scaler** desde los archivos `.pkl`.\n",
    "2. **Prepara datos de prueba**:\n",
    "   - Puedes cargar un CSV con datos reales o simular datos como en el ejemplo.\n",
    "3. **Aplica el mismo preprocesamiento**:\n",
    "   - Escalado numérico con `RobustScaler` (¡igual que en entrenamiento!).\n",
    "4. **Genera predicciones**:\n",
    "   - Convierte los resultados de log(price) a EUR con `np.expm1()`.\n",
    "5. **Muestra resultados**:\n",
    "   - Precios predichos + importancia de cada feature (útil para debug).\n",
    "\n",
    "---\n",
    "\n",
    "### **Ejemplo de salida:**\n",
    "```\n",
    "🔮 Predicciones:\n",
    "Alojamiento 1: $85.50 EUR\n",
    "Alojamiento 2: $120.30 EUR\n",
    "\n",
    "📊 Importancia de features:\n",
    "bedrooms: 0.2543\n",
    "accommodates: 0.1987\n",
    "bathrooms: 0.1852\n",
    "...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Requisitos para ejecutarlo**\n",
    "1. Estructura de directorios:\n",
    "   ```\n",
    "   your_project/\n",
    "   ├── models/\n",
    "   │   ├── random_forest_v5.pkl\n",
    "   │   └── scaler.pkl\n",
    "   └── test_model.py\n",
    "   ```\n",
    "\n",
    "2. Bibliotecas:\n",
    "   ```bash\n",
    "   pip install pandas numpy scikit-learn\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### **Consejos clave**\n",
    "1. **Verifica las features**:  \n",
    "   El DataFrame `X_new` debe tener exactamente las mismas columnas que usaste para entrenar (en mismo orden).\n",
    "\n",
    "2. **Para producción**:  \n",
    "   Guarda también la lista de columnas usadas en entrenamiento:\n",
    "   ```python\n",
    "   # Durante el entrenamiento:\n",
    "   with open(model_dir / \"feature_columns.pkl\", 'wb') as f:\n",
    "       pickle.dump(list(X_train.columns), f)\n",
    "   \n",
    "   # Durante la predicción:\n",
    "   with open(model_dir / \"feature_columns.pkl\", 'rb') as f:\n",
    "       expected_columns = pickle.load(f)\n",
    "   X_new = X_new[expected_columns]  # Reordena columnas\n",
    "   ```\n",
    "\n",
    "3. **Si falla**:  \n",
    "   Revisa que las versiones de `scikit-learn` sean consistentes entre entrenamiento y predicción."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f66d83",
   "metadata": {},
   "source": [
    "## **1. Carga del Modelo.pkl y Generaración de Datos de Prueba Aleatorios y Consistentes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0958d6d9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Aquí tienes una solución completa que:\n",
    "1. **Carga el modelo, scaler y columnas** desde los archivos `.pkl`.\n",
    "2. **Genera datos de prueba aleatorios** basados en estadísticas reales del dataset original.\n",
    "3. **Valida las columnas** para asegurar compatibilidad.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Script (`test_model_with_random_data.py`)**\n",
    "```python\n",
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "# --- Configuración de paths ---\n",
    "PROJECT_ROOT = Path(__file__).parent.parent  # Ajusta según tu estructura\n",
    "MODEL_PATH = PROJECT_ROOT / \"models\" / \"random_forest_v5.pkl\"\n",
    "SCALER_PATH = PROJECT_ROOT / \"models\" / \"scaler.pkl\"\n",
    "FEATURE_COLUMNS_PATH = PROJECT_ROOT / \"models\" / \"feature_columns.pkl\"\n",
    "DATA_PATH = PROJECT_ROOT / \"data\" / \"processed\" / \"df_optimized.csv\"  # CSV original\n",
    "\n",
    "def load_artifacts():\n",
    "    \"\"\"Cargar modelo, scaler y columnas desde archivos .pkl\"\"\"\n",
    "    with open(MODEL_PATH, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    with open(SCALER_PATH, 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "    with open(FEATURE_COLUMNS_PATH, 'rb') as f:\n",
    "        feature_columns = pickle.load(f)\n",
    "    return model, scaler, feature_columns\n",
    "\n",
    "def generate_random_test_data(df_original, feature_columns, n_samples=5):\n",
    "    \"\"\"Genera datos aleatorios basados en estadísticas del dataset original\"\"\"\n",
    "    random_data = {}\n",
    "    for col in feature_columns:\n",
    "        if df_original[col].dtype in ['int64', 'float64']:\n",
    "            # Para numéricas: valor dentro del rango [Q1, Q3] (evita outliers)\n",
    "            q1 = df_original[col].quantile(0.25)\n",
    "            q3 = df_original[col].quantile(0.75)\n",
    "            random_data[col] = [random.uniform(q1, q3) for _ in range(n_samples)]\n",
    "        elif df_original[col].dtype == 'object':\n",
    "            # Para categóricas: muestra aleatoria de valores únicos\n",
    "            random_data[col] = random.choices(df_original[col].dropna().unique(), k=n_samples)\n",
    "        else:\n",
    "            # Para booleanas (ej: host_is_superhost): 0 o 1\n",
    "            random_data[col] = [random.randint(0, 1) for _ in range(n_samples)]\n",
    "    return pd.DataFrame(random_data)\n",
    "\n",
    "def predict_price(model, scaler, X_test):\n",
    "    \"\"\"Preprocesa y predice precios\"\"\"\n",
    "    # Escalar features numéricas (si el scaler existe)\n",
    "    numeric_cols = X_test.select_dtypes(include=['int64', 'float64']).columns\n",
    "    if scaler:\n",
    "        X_test[numeric_cols] = scaler.transform(X_test[numeric_cols])\n",
    "    # Predecir y convertir a EUR\n",
    "    return np.expm1(model.predict(X_test))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Cargar artefactos ---\n",
    "    model, scaler, feature_columns = load_artifacts()\n",
    "    print(f\"🔄 Columnas esperadas: {feature_columns}\")\n",
    "\n",
    "    # --- Generar datos aleatorios consistentes ---\n",
    "    df_original = pd.read_csv(DATA_PATH)\n",
    "    X_test_random = generate_random_test_data(df_original, feature_columns, n_samples=3)\n",
    "    \n",
    "    print(\"\\n🎲 Datos de prueba generados (aleatorios pero consistentes):\")\n",
    "    print(X_test_random)\n",
    "\n",
    "    # --- Predecir ---\n",
    "    prices = predict_price(model, scaler, X_test_random)\n",
    "    for i, price in enumerate(prices):\n",
    "        print(f\"\\n🏠 Predicción {i+1}: ${price:.2f} EUR\")\n",
    "\n",
    "    # --- Opcional: Exportar datos de prueba a CSV ---\n",
    "    X_test_random.to_csv(PROJECT_ROOT / \"data\" / \"test_samples.csv\", index=False)\n",
    "    print(\"\\n💾 Datos guardados en 'data/test_samples.csv'\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **¿Cómo funciona?**\n",
    "1. **Generación de datos aleatorios**:\n",
    "   - **Para features numéricas**: Usa el rango entre el primer y tercer cuartil (Q1-Q3) para evitar outliers.\n",
    "   - **Para categóricas**: Muestra aleatoria de valores únicos del dataset original.\n",
    "   - **Para booleanas**: Aleatoriamente 0 o 1.\n",
    "\n",
    "2. **Validación implícita**:\n",
    "   - Al basarse en `feature_columns.pkl`, garantiza que las columnas y orden sean idénticos a los de entrenamiento.\n",
    "\n",
    "3. **Consistencia**:\n",
    "   - Los datos generados respetan la distribución original (ej: `bedrooms` no será 100 si el máximo original era 5).\n",
    "\n",
    "---\n",
    "\n",
    "### **Ejemplo de salida**\n",
    "```\n",
    "🔄 Columnas esperadas: ['accommodates', 'bathrooms', 'bedrooms', 'beds', ...]\n",
    "\n",
    "🎲 Datos de prueba generados:\n",
    "   accommodates  bathrooms  bedrooms  beds  ...\n",
    "0             2        1.0         1     1\n",
    "1             4        2.0         2     3\n",
    "2             3        1.5         1     2\n",
    "\n",
    "🏠 Predicción 1: $85.50 EUR\n",
    "🏠 Predicción 2: $120.30 EUR\n",
    "🏠 Predicción 3: $92.10 EUR\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Ventajas de este enfoque**\n",
    "1. **Robustez**: Datos aleatorios pero realistas.\n",
    "2. **Debugging fácil**: Puedes guardar los datos generados en CSV (`test_samples.csv`) para replicar pruebas.\n",
    "3. **Preparación para producción**: Si luego quieres probar con datos reales, solo sustituye `X_test_random` por tu DataFrame.\n",
    "\n",
    "---\n",
    "\n",
    "### **¿Cómo personalizarlo?**\n",
    "- **Para ajustar aleatoriedad**: Modifica `generate_random_test_data()`.\n",
    "  - Ejemplo: Si `bathrooms` debe ser múltiplo de 0.5:\n",
    "    ```python\n",
    "    random_data[col] = [round(random.uniform(q1, q3) * 2) / 2 for _ in range(n_samples)]\n",
    "    ```\n",
    "- **Para excluir columnas**: Filtra `feature_columns` antes de usarlo.\n",
    "\n",
    "¿Necesitas ajustar algo más específico para tu caso de uso?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a71d24",
   "metadata": {},
   "source": [
    "## **2. Carga del Modelo.pkl con introducción de datos por usuario en terminal**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c839d4f7",
   "metadata": {},
   "source": [
    "### **Script `test.py` para Ingresar Datos por Terminal (y Futura Integración con React)**\n",
    "\n",
    "Aquí tienes un script que:\n",
    "1. **Carga las columnas del modelo** desde `feature_columns.pkl`.\n",
    "2. **Pide al usuario que ingrese valores para cada columna** (validando tipos de datos).\n",
    "3. **Genera predicciones** y las devuelve en formato JSON (ideal para conectar con React).\n",
    "\n",
    "---\n",
    "\n",
    "#### **Archivo `test.py`**\n",
    "```python\n",
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# --- Configuración de paths ---\n",
    "PROJECT_ROOT = Path(__file__).parent.parent  # Ajusta según tu estructura\n",
    "MODEL_PATH = PROJECT_ROOT / \"models\" / \"random_forest_v5.pkl\"\n",
    "SCALER_PATH = PROJECT_ROOT / \"models\" / \"scaler.pkl\"\n",
    "FEATURE_COLUMNS_PATH = PROJECT_ROOT / \"models\" / \"feature_columns.pkl\"\n",
    "\n",
    "def load_artifacts():\n",
    "    \"\"\"Cargar modelo, scaler y columnas desde archivos .pkl\"\"\"\n",
    "    with open(MODEL_PATH, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    with open(SCALER_PATH, 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "    with open(FEATURE_COLUMNS_PATH, 'rb') as f:\n",
    "        feature_columns = pickle.load(f)\n",
    "    return model, scaler, feature_columns\n",
    "\n",
    "def get_user_input(feature_columns):\n",
    "    \"\"\"Recolecta datos de entrada del usuario vía terminal\"\"\"\n",
    "    input_data = {}\n",
    "    print(\"\\n🛠️  Ingresa los valores para cada feature (presiona Enter para omitir y usar valor por defecto):\")\n",
    "    \n",
    "    # Valores por defecto (basados en medianas o modas)\n",
    "    default_values = {\n",
    "        'accommodates': 2,\n",
    "        'bathrooms': 1.0,\n",
    "        'bedrooms': 1,\n",
    "        'beds': 1,\n",
    "        'minimum_nights': 2,\n",
    "        'number_of_reviews': 10,\n",
    "        'review_scores_rating': 90,\n",
    "        'host_is_superhost': 0,\n",
    "        'neighbourhood_density': 0.5,\n",
    "        'has_wifi': 1,\n",
    "        'has_air_conditioning': 1\n",
    "    }\n",
    "    \n",
    "    for col in feature_columns:\n",
    "        while True:\n",
    "            try:\n",
    "                user_input = input(f\"{col} (default={default_values.get(col, 'N/A')}): \").strip()\n",
    "                if not user_input:  # Si el usuario presiona Enter\n",
    "                    input_data[col] = default_values[col]\n",
    "                    break\n",
    "                # Conversión de tipos\n",
    "                if isinstance(default_values.get(col), float):\n",
    "                    input_data[col] = float(user_input)\n",
    "                elif isinstance(default_values.get(col), int):\n",
    "                    input_data[col] = int(user_input)\n",
    "                else:\n",
    "                    input_data[col] = user_input\n",
    "                break\n",
    "            except ValueError:\n",
    "                print(f\"⚠️ Error: Ingresa un valor válido para {col} (ej: {default_values.get(col)})\")\n",
    "    \n",
    "    return pd.DataFrame([input_data])\n",
    "\n",
    "def predict_price(model, scaler, input_data):\n",
    "    \"\"\"Preprocesa y predice precios\"\"\"\n",
    "    # Escalar features numéricas\n",
    "    numeric_cols = input_data.select_dtypes(include=['int64', 'float64']).columns\n",
    "    if scaler:\n",
    "        input_data[numeric_cols] = scaler.transform(input_data[numeric_cols])\n",
    "    # Predecir y convertir a EUR\n",
    "    return np.expm1(model.predict(input_data))[0]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Cargar modelo y columnas ---\n",
    "    model, scaler, feature_columns = load_artifacts()\n",
    "    print(f\"🔍 Columnas requeridas: {feature_columns}\")\n",
    "    \n",
    "    # --- Obtener datos del usuario ---\n",
    "    user_data_df = get_user_input(feature_columns)\n",
    "    \n",
    "    # --- Predecir y mostrar resultado ---\n",
    "    predicted_price = predict_price(model, scaler, user_data_df)\n",
    "    print(f\"\\n🎯 Precio predicho: ${predicted_price:.2f} EUR\")\n",
    "    \n",
    "    # --- Salida en JSON (para integrar con React) ---\n",
    "    output_json = {\n",
    "        \"input_data\": user_data_df.iloc[0].to_dict(),\n",
    "        \"predicted_price\": round(float(predicted_price), 2)\n",
    "    }\n",
    "    print(\"\\n📤 JSON para React (copiar esto):\")\n",
    "    print(json.dumps(output_json, indent=2))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Cómo Usarlo**\n",
    "1. **Ejecuta el script**:\n",
    "   ```bash\n",
    "   python test.py\n",
    "   ```\n",
    "\n",
    "2. **Ingresa valores** (o presiona Enter para usar valores por defecto):\n",
    "   ```\n",
    "   accommodates (default=2): 3\n",
    "   bathrooms (default=1.0): 1.5\n",
    "   bedrooms (default=1): \n",
    "   ...\n",
    "   ```\n",
    "\n",
    "3. **Resultado en terminal**:\n",
    "   ```json\n",
    "   {\n",
    "     \"input_data\": {\n",
    "       \"accommodates\": 3,\n",
    "       \"bathrooms\": 1.5,\n",
    "       \"bedrooms\": 1,\n",
    "       ...\n",
    "     },\n",
    "     \"predicted_price\": 120.50\n",
    "   }\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### **Claves para la Integración con React**\n",
    "1. **Formato JSON**:  \n",
    "   El output está listo para ser consumido por tu frontend (ej: mediante `fetch` o Axios).\n",
    "\n",
    "2. **Valores por defecto**:  \n",
    "   Usa medianas/modes del dataset para evitar inputs vacíos.\n",
    "\n",
    "3. **Validación de tipos**:  \n",
    "   El script fuerza `int`/`float` según lo que el modelo espera.\n",
    "\n",
    "4. **Escalabilidad**:  \n",
    "   Si añades más features, solo actualiza `default_values`.\n",
    "\n",
    "---\n",
    "\n",
    "### **Adaptación para API (Opcional)**\n",
    "Si prefieres llamar al script desde React como una API, usa `Flask`:\n",
    "\n",
    "#### **Archivo `api.py`** (en la misma carpeta que `test.py`)\n",
    "```python\n",
    "from flask import Flask, request, jsonify\n",
    "import subprocess\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    data = request.json\n",
    "    # Ejecuta test.py y captura el output\n",
    "    result = subprocess.run(['python', 'test.py'], input=json.dumps(data), capture_output=True, text=True)\n",
    "    return jsonify(json.loads(result.stdout))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(port=5000)\n",
    "```\n",
    "\n",
    "En React:\n",
    "```javascript\n",
    "fetch('http://localhost:5000/predict', {\n",
    "  method: 'POST',\n",
    "  headers: { 'Content-Type': 'application/json' },\n",
    "  body: JSON.stringify({ accommodates: 2, bathrooms: 1.5, ... })\n",
    "})\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Notas Finales**\n",
    "- **Seguridad**: Si el frontend y backend están separados, habilita CORS en Flask.\n",
    "- **Performance**: Para producción, carga el modelo una vez al iniciar la API (no en cada request).\n",
    "- **Testing**: Usa `pytest` para automatizar pruebas con diferentes inputs.\n",
    "\n",
    "¿Necesitas ajustar algo más para tu implementación específica?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8e6fb1",
   "metadata": {},
   "source": [
    "## **Si necesitas volver a entrenar el modelo**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6cb547",
   "metadata": {},
   "source": [
    "Para que todo funcione correctamente, **sí necesitas volver a ejecutar el script de entrenamiento** (`train_model.py`) y asegurarte de que guarde el archivo `feature_columns.pkl` con las columnas correctas. Aquí te detallo los pasos:\n",
    "\n",
    "---\n",
    "\n",
    "### **Pasos a Seguir**\n",
    "\n",
    "1. **Ejecuta el script de entrenamiento**:\n",
    "   ```bash\n",
    "   python train_model.py\n",
    "   ```\n",
    "   Esto generará (o actualizará) los archivos:\n",
    "   - `models/random_forest_v5.pkl` (modelo).\n",
    "   - `models/scaler.pkl` (scaler).\n",
    "   - `models/feature_columns.pkl` (columnas usadas).\n",
    "\n",
    "2. **Verifica que el archivo `feature_columns.pkl` existe**:\n",
    "   ```bash\n",
    "   ls models/feature_columns.pkl\n",
    "   ```\n",
    "   Si no aparece, revisa que tu script de entrenamiento incluya este bloque:\n",
    "   ```python\n",
    "   # En train_model.py (justo después de definir X_train)\n",
    "   with open(model_dir / \"feature_columns.pkl\", 'wb') as f:\n",
    "       pickle.dump(list(X_train.columns), f)\n",
    "   ```\n",
    "\n",
    "3. **Prueba los scripts de predicción**:\n",
    "   - **Para datos aleatorios**:\n",
    "     ```bash\n",
    "     python test_model_with_random_data.py\n",
    "     ```\n",
    "   - **Para entrada manual por terminal**:\n",
    "     ```bash\n",
    "     python test.py\n",
    "     ```\n",
    "\n",
    "---\n",
    "\n",
    "### **¿Por qué es necesario?**\n",
    "- **Consistencia**: El archivo `feature_columns.pkl` asegura que los scripts de predicción usen **exactamente las mismas columnas** que el modelo espera (en el mismo orden).\n",
    "- **Evita errores**: Si no lo regeneras, podrías tener discrepancias si:\n",
    "  - Modificaste las features en el dataset original (`df_optimized.csv`).\n",
    "  - Cambiaste el preprocesamiento en `train_model.py`.\n",
    "\n",
    "---\n",
    "\n",
    "### **Consejos Adicionales**\n",
    "1. **Si ya entrenaste el modelo previamente**:\n",
    "   - Borra los archivos antiguos en `models/` antes de reentrenar:\n",
    "     ```bash\n",
    "     rm models/*.pkl\n",
    "     ```\n",
    "\n",
    "2. **Para debuggear**:\n",
    "   - Imprime las columnas justo antes de guardarlas:\n",
    "     ```python\n",
    "     print(\"Columnas guardadas:\", list(X_train.columns))\n",
    "     ```\n",
    "\n",
    "3. **Si usas Git**:\n",
    "   - Asegúrate de que `models/feature_columns.pkl` esté en tu `.gitignore` si el modelo pesa mucho (y documenta cómo generarlo).\n",
    "\n",
    "---\n",
    "\n",
    "### **Ejemplo de Salida Esperada**\n",
    "Al ejecutar `train_model.py`, deberías ver algo como:\n",
    "```\n",
    "✅ Modelo, scaler y columnas guardados en: models/\n",
    "Columnas guardadas: ['accommodates', 'bathrooms', ..., 'has_air_conditioning']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Integración con React (Recordatorio)**\n",
    "Cuando conectes el script `test.py` con tu frontend:\n",
    "1. **Desde React**, llama al script via API (ej: con Flask/Express) o subproceso.\n",
    "2. **Pasa los inputs** como JSON (como ya hiciste en el script).\n",
    "3. **Asegúrate** de que las columnas en React coincidan con las de `feature_columns.pkl`.\n",
    "\n",
    "¿Necesitas ayuda para ajustar algo más en el script de entrenamiento o en la conexión con React?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
