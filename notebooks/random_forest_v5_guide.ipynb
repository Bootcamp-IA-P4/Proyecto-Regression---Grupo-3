{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4acb64eb",
   "metadata": {},
   "source": [
    "# **GuÃ­a de como Entrenar el Modelo, Guardarlo en un archivo pickle y como usarlo**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c784f42",
   "metadata": {},
   "source": [
    "Para transformar el archivo `inside_airbnb_model.py` (que originalmente era un notebook) en un script Python que cargue el archivo `df_optimized.csv`, entrene el modelo v5 y lo guarde en formato pickle, sigue estos pasos:\n",
    "\n",
    "### **Script Python (`train_model_v5.py`)**\n",
    "```python\n",
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# --- ConfiguraciÃ³n de paths ---\n",
    "processed_data_dir = Path(\"../data/processed/\")\n",
    "model_dir = Path(\"../models/\")\n",
    "model_dir.mkdir(parents=True, exist_ok=True)  # Crear directorio si no existe\n",
    "\n",
    "# --- 1. Cargar datos procesados ---\n",
    "df_optimized = pd.read_csv(processed_data_dir / \"df_optimized.csv\")\n",
    "\n",
    "# --- 2. TransformaciÃ³n logarÃ­tmica del target (price) ---\n",
    "y = np.log1p(df_optimized['price'])\n",
    "\n",
    "# --- 3. Definir features (X) ---\n",
    "# Eliminar columnas no relevantes para el modelo\n",
    "X = df_optimized.drop(columns=['price', 'host_since_year'])  # Ajusta segÃºn tu CSV\n",
    "\n",
    "# --- 4. Escalado numÃ©rico (opcional, pero recomendado) ---\n",
    "numeric_features = ['accommodates', 'bathrooms', 'bedrooms', 'beds', 'minimum_nights', 'number_of_reviews']\n",
    "scaler = RobustScaler()\n",
    "X[numeric_features] = scaler.fit_transform(X[numeric_features])\n",
    "\n",
    "# --- 5. DivisiÃ³n train-test (para evaluaciÃ³n) ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- 6. Entrenar modelo Random Forest v5 ---\n",
    "model_v5 = RandomForestRegressor(\n",
    "    n_estimators=300,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=2,\n",
    "    max_features=0.5,\n",
    "    max_samples=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "model_v5.fit(X_train, y_train)\n",
    "\n",
    "# --- 7. EvaluaciÃ³n del modelo ---\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    y_true_exp = np.expm1(y_true)\n",
    "    y_pred_exp = np.expm1(y_pred)\n",
    "    r2 = r2_score(y_true_exp, y_pred_exp)\n",
    "    mae = mean_absolute_error(y_true_exp, y_pred_exp)\n",
    "    print(f\"\\nðŸ“Š **MÃ©tricas para {dataset_name}**\")\n",
    "    print(f\"RÂ²: {r2:.4f} | MAE: {mae:.2f} EUR\")\n",
    "\n",
    "y_pred_train = model_v5.predict(X_train)\n",
    "y_pred_test = model_v5.predict(X_test)\n",
    "print_metrics(y_train, y_pred_train, \"ENTRENAMIENTO\")\n",
    "print_metrics(y_test, y_pred_test, \"PRUEBA\")\n",
    "\n",
    "# --- 8. Guardar modelo y scaler en pickle ---\n",
    "with open(model_dir / \"model_v5.pkl\", 'wb') as f:\n",
    "    pickle.dump(model_v5, f)\n",
    "\n",
    "with open(model_dir / \"scaler.pkl\", 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(f\"\\nâœ… Modelo y scaler guardados en: {model_dir}/\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **ExplicaciÃ³n Paso a Paso**\n",
    "1. **Carga de datos**:  \n",
    "   - Lee el archivo `df_optimized.csv` desde la ruta `../data/processed/`.\n",
    "\n",
    "2. **Preprocesamiento**:  \n",
    "   - Aplica transformaciÃ³n logarÃ­tmica a `price` para manejar colas largas.  \n",
    "   - Escala features numÃ©ricas con `RobustScaler` (protege contra outliers).\n",
    "\n",
    "3. **Entrenamiento del modelo**:  \n",
    "   - Usa los hiperparÃ¡metros Ã³ptimos del **modelo v5** (`n_estimators=300`, `max_features=0.5`, etc.).  \n",
    "   - EvalÃºa con mÃ©tricas en escala original (EUR).\n",
    "\n",
    "4. **ExportaciÃ³n a pickle**:  \n",
    "   - Guarda el modelo entrenado (`model_v5.pkl`) y el scaler (`scaler.pkl`) en la carpeta `../models/`.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "### **Notas Clave**\n",
    "- **Ajusta las columnas**: Verifica que `X` no incluya columnas irrelevantes o con leakage (como `log_price`).  \n",
    "- **Entorno reproducible**: Usa `random_state=42` para resultados consistentes.  \n",
    "- **Requisitos**: AsegÃºrate de tener las mismas versiones de librerÃ­as (`scikit-learn`, `pandas`, etc.) en entrenamiento y producciÃ³n.  \n",
    "\n",
    "Si necesitas adaptar algo especÃ­fico (como las features usadas), modifica el script segÃºn tu caso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f67ca72",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbc038a",
   "metadata": {},
   "source": [
    "## **Como usar el Modelo**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f41a56",
   "metadata": {},
   "source": [
    "Basado en el script de entrenamiento, aquÃ­ tienes cÃ³mo probar correctamente el modelo guardado (`random_forest_v5.pkl`):\n",
    "\n",
    "### **Script completo para probar el modelo (`test_model.py`)**\n",
    "```python\n",
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# --- ConfiguraciÃ³n de paths ---\n",
    "model_dir = Path(\"models/\")\n",
    "processed_data_dir = Path(\"data/processed/\")\n",
    "\n",
    "# --- 1. Cargar modelo y scaler ---\n",
    "with open(model_dir / \"random_forest_v5.pkl\", 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "    \n",
    "with open(model_dir / \"scaler.pkl\", 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "\n",
    "# --- 2. Cargar datos de prueba (o nuevos datos) ---\n",
    "# OpciÃ³n A: Usar datos de prueba guardados (si existen)\n",
    "# df_test = pd.read_csv(processed_data_dir / \"test_data.csv\")\n",
    "\n",
    "# OpciÃ³n B: Crear datos de ejemplo (simulando entrada)\n",
    "test_data = {\n",
    "    'accommodates': [2, 4],  # Ejemplo: 2 personas\n",
    "    'bathrooms': [1.0, 2.0],\n",
    "    'bedrooms': [1, 2],\n",
    "    'beds': [1, 3],\n",
    "    'minimum_nights': [3, 2],\n",
    "    'number_of_reviews': [10, 25],\n",
    "    'review_scores_rating': [95, 80],\n",
    "    'host_is_superhost': [1, 0],  # 1=True, 0=False\n",
    "    'neighbourhood_density': [0.5, 0.3],\n",
    "    'has_wifi': [1, 1],\n",
    "    'has_air_conditioning': [1, 0],\n",
    "    # AÃ±ade todas las features usadas en entrenamiento\n",
    "}\n",
    "\n",
    "X_new = pd.DataFrame(test_data)\n",
    "\n",
    "# --- 3. Preprocesamiento igual que en entrenamiento ---\n",
    "# Escalar features numÃ©ricas\n",
    "numeric_features = ['accommodates', 'bathrooms', 'bedrooms', 'beds', \n",
    "                   'minimum_nights', 'number_of_reviews']\n",
    "X_new[numeric_features] = scaler.transform(X_new[numeric_features])\n",
    "\n",
    "# --- 4. Predecir ---\n",
    "pred_log = model.predict(X_new)  # Predicciones en escala logarÃ­tmica\n",
    "pred_price = np.expm1(pred_log)  # Convertir a precio real (EUR)\n",
    "\n",
    "# --- 5. Resultados ---\n",
    "print(\"\\nðŸ”® Predicciones:\")\n",
    "for i, price in enumerate(pred_price):\n",
    "    print(f\"Alojamiento {i+1}: ${price:.2f} EUR\")\n",
    "\n",
    "# --- 6. Opcional: Importancia de features ---\n",
    "if hasattr(model, 'feature_importances_'):\n",
    "    print(\"\\nðŸ“Š Importancia de features:\")\n",
    "    features = X_new.columns\n",
    "    importances = model.feature_importances_\n",
    "    for feat, imp in sorted(zip(features, importances), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"{feat}: {imp:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Â¿QuÃ© hace este script?**\n",
    "1. **Carga el modelo y scaler** desde los archivos `.pkl`.\n",
    "2. **Prepara datos de prueba**:\n",
    "   - Puedes cargar un CSV con datos reales o simular datos como en el ejemplo.\n",
    "3. **Aplica el mismo preprocesamiento**:\n",
    "   - Escalado numÃ©rico con `RobustScaler` (Â¡igual que en entrenamiento!).\n",
    "4. **Genera predicciones**:\n",
    "   - Convierte los resultados de log(price) a EUR con `np.expm1()`.\n",
    "5. **Muestra resultados**:\n",
    "   - Precios predichos + importancia de cada feature (Ãºtil para debug).\n",
    "\n",
    "---\n",
    "\n",
    "### **Ejemplo de salida:**\n",
    "```\n",
    "ðŸ”® Predicciones:\n",
    "Alojamiento 1: $85.50 EUR\n",
    "Alojamiento 2: $120.30 EUR\n",
    "\n",
    "ðŸ“Š Importancia de features:\n",
    "bedrooms: 0.2543\n",
    "accommodates: 0.1987\n",
    "bathrooms: 0.1852\n",
    "...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Requisitos para ejecutarlo**\n",
    "1. Estructura de directorios:\n",
    "   ```\n",
    "   your_project/\n",
    "   â”œâ”€â”€ models/\n",
    "   â”‚   â”œâ”€â”€ random_forest_v5.pkl\n",
    "   â”‚   â””â”€â”€ scaler.pkl\n",
    "   â””â”€â”€ test_model.py\n",
    "   ```\n",
    "\n",
    "2. Bibliotecas:\n",
    "   ```bash\n",
    "   pip install pandas numpy scikit-learn\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### **Consejos clave**\n",
    "1. **Verifica las features**:  \n",
    "   El DataFrame `X_new` debe tener exactamente las mismas columnas que usaste para entrenar (en mismo orden).\n",
    "\n",
    "2. **Para producciÃ³n**:  \n",
    "   Guarda tambiÃ©n la lista de columnas usadas en entrenamiento:\n",
    "   ```python\n",
    "   # Durante el entrenamiento:\n",
    "   with open(model_dir / \"feature_columns.pkl\", 'wb') as f:\n",
    "       pickle.dump(list(X_train.columns), f)\n",
    "   \n",
    "   # Durante la predicciÃ³n:\n",
    "   with open(model_dir / \"feature_columns.pkl\", 'rb') as f:\n",
    "       expected_columns = pickle.load(f)\n",
    "   X_new = X_new[expected_columns]  # Reordena columnas\n",
    "   ```\n",
    "\n",
    "3. **Si falla**:  \n",
    "   Revisa que las versiones de `scikit-learn` sean consistentes entre entrenamiento y predicciÃ³n."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
